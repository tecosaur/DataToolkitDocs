[{"id":3,"pagetitle":"Introduction","title":"Introduction","ref":"/DataToolkitDocs/ref/stable/#Introduction","content":" Introduction DataToolkit  is a batteries-included framework for robustly managing data. For details on the fundamental system, see the  DataToolkitBase  docs; for details on the transformers and plugins available see the  DataToolkitCommon  docs."},{"id":4,"pagetitle":"Introduction","title":"Why this exists","ref":"/DataToolkitDocs/ref/stable/#Why-this-exists","content":" Why this exists Poor data management approaches are rampant. This is well-recognised, and so people have created tools that attempt to tackle subsets of the problem — such as  DataLad ,  DVC , the  Kedro data catalogue ,  Snakemake ,  Nextflow ,  Intake ,  Pkg.jl's Artifacts , and  DataSets.jl . These tools contain many good ideas, but all fall short of the combination of  convenience  and  robustness  that is possible. DataToolkit leverages key Julia features —reproducible package management with Pkg.jl, independence from system state with JLL packages, and well-managed environments— to push the envelope on how easily data can be robustly managed. The three tenets of the project are  reproducibility ,  flexibility , and  convenience ."},{"id":5,"pagetitle":"Introduction","title":"Declarative data management","ref":"/DataToolkitDocs/ref/stable/#Declarative-data-management","content":" Declarative data management DataToolkit takes a declarative approach to data management, and represents collections of datasets in TOML files. To give a taste of what this system looks like in practice, here's a sample TOML representation of a dataset ( iris ). [[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    checksum = \"crc32c:d5c06b86\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\"\n    args.header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"]\n    args.skipto = 2"},{"id":6,"pagetitle":"Introduction","title":"The data model","ref":"/DataToolkitDocs/ref/stable/#The-data-model","content":" The data model"},{"id":7,"pagetitle":"Introduction","title":"Data Sets — Information and Data","ref":"/DataToolkitDocs/ref/stable/#Data-Sets-—-Information-and-Data","content":" Data Sets — Information and Data Useful  information  is a particular representation of basic  data . We acquire data and  load  it into a more informative form, and similarly can  write  information back as data. To give a concrete example, Fisher's famous iris data set can exist on-disk as a CSV, comprised of bytes or ASCII characters. This is the  data  form. If we want to do useful analysis, we would want to transform the data into say a table of information (e.g. as a  DataFrame ). This is the  information  form. We can  load  the information form by parsing the CSV data, and  write  the information back by serialising the table. There are thus three essential transformations that can occur involving a data set: The transformation of a specification into a source of  data , termed storage The transformation of  data  into  information , termed a  loader The transformation of  information  into  data , termed a  writer"},{"id":8,"pagetitle":"Introduction","title":"Transformers, Data Sets, and Data Collections","ref":"/DataToolkitDocs/ref/stable/#Transformers,-Data-Sets,-and-Data-Collections","content":" Transformers, Data Sets, and Data Collections Each  DataSet  can have any number of  storage ,  loader , and  writer  transformers. All  DataSet ​s must be part of a  DataCollection  which essentially provides a context for the existence of a particular data set (e.g. you might store Fisher's iris data under a \"Flower measurements\" data collection). The  DataCollection ​s loaded at any one time form the  DataCollection stack . The stack essentially acts as a load-path, if you just ask for the  iris  data set, it will be fetched from the top collection on the stack that can satisfy it. It is also worth noting that \"child\" elements of this structure (data sets and transformers) contain a link back to their parent, and so from any part of a  DataCollection  the whole can be accessed."},{"id":9,"pagetitle":"Introduction","title":"Extreme extensibility","ref":"/DataToolkitDocs/ref/stable/#Extreme-extensibility","content":" Extreme extensibility The plethora of formats, workflows, and tools that surround data make designing a \"do it all\" system implausible. A much easier task is to produce a system that can be  adapted  to serve as many use cases as possible, even ones the designers have never conceived of! To that end, extensibility is weaved throughout the code base. The core system ( DataToolkitBase ) is generic to the point of being useless on its own, and special-case behaviour has been avoided. Many basic features (such as default values) are implemented as plugins, to avoid being inadvertently privileged in the core system. I have yet to come across a feature that could not be implemented under this framework."},{"id":12,"pagetitle":"Data.toml format","title":"Data.toml","ref":"/DataToolkitDocs/ref/stable/datatoml/#Data.toml","content":" Data.toml Data collections are represented on-disk as  Data.toml  files. While  DataToolkit  can be used at a basic level without any knowledge of the structure of the file, a little knowledge goes a long way (for instance when  edit ​ing a dataset)."},{"id":13,"pagetitle":"Data.toml format","title":"Overall structure","ref":"/DataToolkitDocs/ref/stable/datatoml/#Overall-structure","content":" Overall structure See the TOML refresher below if you're a bit rusty, then come back to this. A  Data.toml  file is broadly composed of three sections: Global setup information Configuration Datasets Here's what that structure looks like in practice: data_config_version=0\n\nname=\"data collection name\"\nuuid=\"a UUIDv4\"\nplugins=[\"plugin1\", \"plugin2\", ...]\n\n[config]\n# [Properties of the data collection itself]\n\n[[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties...\n\n[[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\ntype=[\"a QualifiedType\", ...]\npriority=1 # (optional)\n# other properties...\n\n[[mydataset]]\n# There may be multiple data sets by the same name,\n# but they must be uniquely identifyable by their properties\n\n[[exampledata]]\n# Another data set"},{"id":14,"pagetitle":"Data.toml format","title":"Global setup","ref":"/DataToolkitDocs/ref/stable/datatoml/#Global-setup","content":" Global setup The global setup must specify: The  Data.toml  format version The name and UUID of the data collection The plugins used by the data collection"},{"id":15,"pagetitle":"Data.toml format","title":"Configuration","ref":"/DataToolkitDocs/ref/stable/datatoml/#Configuration","content":" Configuration The  config  TOML table is special, and is used to hold custom attributes of the data collection, for example: [config]\nmykey=\"value\"\n\n[config.defaults]\ndescription=\"Ooops, somebody forgot to describe this.\"\n\n[config.defaults.storage.filesystem]\npriority=2 As a consequence of this, no dataset may be named  \"config\" ."},{"id":16,"pagetitle":"Data.toml format","title":"Datasets","ref":"/DataToolkitDocs/ref/stable/datatoml/#Datasets","content":" Datasets All datasets are represented using an  array of tables . This allows multiple datasets to have the same name, and be distinguished by other attributes (e.g. version information). All datasets must have a  uuid  key, this is important for providing a canonical unique reference to a particular dataset. [[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties... The storage/loader/writer transformers of a dataset are specified using sub-tables, i.e. [[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\n# other properties... All transformers  must  set the  driver  key. All attributes other than  driver ,  type , and  priority  are free to be used by the transformer and plugins."},{"id":17,"pagetitle":"Data.toml format","title":"TOML refresher","ref":"/DataToolkitDocs/ref/stable/datatoml/#TOML-refresher","content":" TOML refresher TOML files are already widely used with Julia (for example,  Project.toml  and  Manifest.toml ) files, as they strike a good compromise between capability and complexity. See  the TOML documentation  for a full description of the format, but here are the components most relevant to  Data.toml  files."},{"id":18,"pagetitle":"Data.toml format","title":"Key-value pairs","ref":"/DataToolkitDocs/ref/stable/datatoml/#Key-value-pairs","content":" Key-value pairs key = \"value\" This represents a  \"key\"  dictionary key having the value  \"value\" . Strings, numbers, booleans, and date/time stamps are all appropriate value forms. a = \"value\"\nb = 2\nc = 3.1e+12\nd = true\ne = 1979-05-27T07:32:00Z Arrays are written using  [ ]  syntax, and can spread across multiple lines. key = [1, 2, 3]"},{"id":19,"pagetitle":"Data.toml format","title":"Tables (Dictionaries)","ref":"/DataToolkitDocs/ref/stable/datatoml/#Tables-(Dictionaries)","content":" Tables (Dictionaries) A collection of key-value pairs within a certain scope form a Julia  Dict  when parsed. TOML allows for nested dictionaries using  tables . A new table is created with a bracketed header line, like so: [new_table] All key-value entries after such a table header, up to the next table header, belong to that table. For example: [mytable]\na = 1\nb = 2 this is parsed as Dict(\"mytable\" => Dict(\"a\" => 1, \"b\" => 2)) It is also possible to represent this using dotted keys, e.g. mytable.a = 1\nmytable.b = 2 These two styles can mixed to form nested tables. [mytable.innertable.deeply_nested]\nkey = \"value\""},{"id":20,"pagetitle":"Data.toml format","title":"Arrays of tables","ref":"/DataToolkitDocs/ref/stable/datatoml/#Arrays-of-tables","content":" Arrays of tables A list of dictionaries (array of tables in TOML terminology) can be formed using double-bracketed headers, e.g. [[table_array]] All double-bracketed tables will be collected together into an array, for example: [[table_array]]\nkey = 1\n\n[[table_array]]\nkey = 2 will be parsed as Dict(\"table_array\" => [Dict(\"key\" => 1),\n                       Dict(\"key\" => 2)])"},{"id":23,"pagetitle":"Quick Reference Guide","title":"Quick Reference Guide","ref":"/DataToolkitDocs/ref/stable/quickref/#Quick-Reference-Guide","content":" Quick Reference Guide This gives the bare essentials, as the relevant Data REPL command (enter the Data REPL with  } ) and Julia function when sensible."},{"id":24,"pagetitle":"Quick Reference Guide","title":"Data REPL help","ref":"/DataToolkitDocs/ref/stable/quickref/#Data-REPL-help","content":" Data REPL help Look at the REPL  help  docs, accessible within the Data REPL. (demo) data> help help"},{"id":25,"pagetitle":"Quick Reference Guide","title":"Accessing a dataset","ref":"/DataToolkitDocs/ref/stable/quickref/#Accessing-a-dataset","content":" Accessing a dataset"},{"id":26,"pagetitle":"Quick Reference Guide","title":"In the default form","ref":"/DataToolkitDocs/ref/stable/quickref/#In-the-default-form","content":" In the default form Using the Data REPL. (demo) data> show <identifier> Within a program. d\"<identifier>\"\nread(dataset(\"<identifier>\"))"},{"id":27,"pagetitle":"Quick Reference Guide","title":"As a particular type","ref":"/DataToolkitDocs/ref/stable/quickref/#As-a-particular-type","content":" As a particular type Either add  ::<type>  to the identifier string, or if using  read  provide the type as the second argument, i.e. read(dataset(\"<identifier>\"), TYPE)"},{"id":28,"pagetitle":"Quick Reference Guide","title":"Creating a new dataset","ref":"/DataToolkitDocs/ref/stable/quickref/#Creating-a-new-dataset","content":" Creating a new dataset Using the Data REPL. (demo) data> add <name> <source> Within a program. DataToolkit.Base.add(DataSet, \"<name>\", Dict{String, Any}(), \"<source>\"; ...)"},{"id":29,"pagetitle":"Quick Reference Guide","title":"Loading a data collection","ref":"/DataToolkitDocs/ref/stable/quickref/#Loading-a-data-collection","content":" Loading a data collection Using the Data REPL (⋅) data> stack load <path> Within a program. loadcollection!(\"<path>\")"},{"id":30,"pagetitle":"Quick Reference Guide","title":"Creating a data collection","ref":"/DataToolkitDocs/ref/stable/quickref/#Creating-a-data-collection","content":" Creating a data collection Using the Data REPL. (⋅) data> init Within a program. DataToolkit.init()"},{"id":31,"pagetitle":"Quick Reference Guide","title":"Using a package within a  julia  loader script","ref":"/DataToolkitDocs/ref/stable/quickref/#Using-a-package-within-a-julia-loader-script","content":" Using a package within a  julia  loader script Use  @import ...  instead of  import ...  (and don't use  using )."},{"id":32,"pagetitle":"Quick Reference Guide","title":"Registering a package for use with  @import","ref":"/DataToolkitDocs/ref/stable/quickref/#Registering-a-package-for-use-with-@import","content":" Registering a package for use with  @import Call  DataToolkit.@addpkgs A B C... , or to make all direct dependencies of the current module available:  DataToolkit.@addpkgs * ."},{"id":33,"pagetitle":"Quick Reference Guide","title":"Using the Data REPL within code","ref":"/DataToolkitDocs/ref/stable/quickref/#Using-the-Data-REPL-within-code","content":" Using the Data REPL within code The  cmd  macro  data ...`` allows for Data REPL commands to be easily inserted within a program. This also makes it relatively simple to invoke Data REPL functions from the shell. ~$ julia -e 'using DataToolkit; data`stuff...`'"},{"id":36,"pagetitle":"Reference","title":"Reference","ref":"/DataToolkitDocs/ref/stable/reference/#Reference","content":" Reference This is the public API for DataToolkit. Some symbols have been exported for convenience, others need to be specifically imported or accessed with  DataToolkit.<thing> ."},{"id":37,"pagetitle":"Reference","title":"Exported Symbols","ref":"/DataToolkitDocs/ref/stable/reference/#Exported-Symbols","content":" Exported Symbols"},{"id":38,"pagetitle":"Reference","title":"Macros","ref":"/DataToolkitDocs/ref/stable/reference/#Macros","content":" Macros"},{"id":39,"pagetitle":"Reference","title":"DataToolkit","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit","content":" DataToolkit"},{"id":40,"pagetitle":"Reference","title":"DataToolkit.@d_str","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit.@d_str","content":" DataToolkit.@d_str  —  Macro @d_str -> loaded data Shorthand for loading a dataset in the default format,  d\"iris\"  is equivalent to  read(dataset(\"iris\")) . source"},{"id":41,"pagetitle":"Reference","title":"DataToolkit.@data_cmd","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit.@data_cmd","content":" DataToolkit.@data_cmd  —  Macro @data_cmd -> Data REPL command result Proxy for running the command in the Data REPL, e.g.  data`config set demo 1`  is equivalent to  data> config set demo 1 . source"},{"id":42,"pagetitle":"Reference","title":"DataToolkitBase","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkitBase","content":" DataToolkitBase Missing docstring. Missing docstring for  @import . Check Documenter's build log for details."},{"id":43,"pagetitle":"Reference","title":"Functions","ref":"/DataToolkitDocs/ref/stable/reference/#Functions","content":" Functions"},{"id":44,"pagetitle":"Reference","title":"DataToolkitBase","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkitBase-2","content":" DataToolkitBase Missing docstring. Missing docstring for  dataset . Check Documenter's build log for details. Missing docstring. Missing docstring for  loadcollection! . Check Documenter's build log for details."},{"id":45,"pagetitle":"Reference","title":"Types","ref":"/DataToolkitDocs/ref/stable/reference/#Types","content":" Types"},{"id":46,"pagetitle":"Reference","title":"DataToolkitBase","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkitBase-3","content":" DataToolkitBase Missing docstring. Missing docstring for  DataSet . Check Documenter's build log for details."},{"id":47,"pagetitle":"Reference","title":"Unexported Symbols","ref":"/DataToolkitDocs/ref/stable/reference/#Unexported-Symbols","content":" Unexported Symbols"},{"id":48,"pagetitle":"Reference","title":"Modules","ref":"/DataToolkitDocs/ref/stable/reference/#Modules","content":" Modules DataToolkitBase  and  DataToolkitCommon  are available as  Base  and  Common  respectively."},{"id":49,"pagetitle":"Reference","title":"Macros","ref":"/DataToolkitDocs/ref/stable/reference/#Macros-2","content":" Macros"},{"id":50,"pagetitle":"Reference","title":"DataToolkit","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit-2","content":" DataToolkit"},{"id":51,"pagetitle":"Reference","title":"DataToolkit.@addpkgs","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit.@addpkgs","content":" DataToolkit.@addpkgs  —  Macro @addpkgs pkgs... For each named package, register it with  DataToolkitBase . Each package must be a dependency of the current module, recorded in its Project.toml. This allows the packages to be used with  DataToolkitBase.@import . Instead of providing a list of packages, the symbol  *  can be provided to register all dependencies. This must be run at runtime to take effect, so be sure to place it in the  __init__  function of a package. Examples @addpkgs JSON3 CSV\n@addpkgs * # Register all dependencies source Missing docstring. Missing docstring for  DataToolkit.@addpkg . Check Documenter's build log for details."},{"id":52,"pagetitle":"Reference","title":"Functions","ref":"/DataToolkitDocs/ref/stable/reference/#Functions-2","content":" Functions"},{"id":53,"pagetitle":"Reference","title":"DataToolkit","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit-3","content":" DataToolkit"},{"id":54,"pagetitle":"Reference","title":"DataToolkit.init","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit.init","content":" DataToolkit.init  —  Function init(mod::Module=Main; force::Bool=false) Load the  mod -local  Data.toml  if it exists. When  mod  is  Main , every  Data.toml  on the load path is loaded, except for  Data.toml s within packages' projects. Unless  force  is set, the data collection is soft-loaded. A  Data.d  directory can be used in place of a  Data.toml , in which case every toml file within it will be read. Mixing  Data.d/*.toml  and  Data.toml  is discouraged. source"},{"id":55,"pagetitle":"Reference","title":"DataToolkit.plugins","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit.plugins","content":" DataToolkit.plugins  —  Function plugins() List the currently availible plugins, by name. source"},{"id":56,"pagetitle":"Reference","title":"DataToolkit.addpkgs","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkit.addpkgs","content":" DataToolkit.addpkgs  —  Function addpkgs(mod::Module, pkgs::Vector{Symbol}) For each package in  pkgs , which are dependencies recorded in  mod 's Project.toml, register the package with  DataToolkitBase.addpkg . If  pkgs  consists of the single symbol  :* , then all dependencies of  mod  will be registered. This must be run at runtime to take effect, so be sure to place it in the  __init__  function of a package. source"},{"id":57,"pagetitle":"Reference","title":"DataToolkitBase","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkitBase-4","content":" DataToolkitBase Missing docstring. Missing docstring for  DataToolkit.getlayer . Check Documenter's build log for details."},{"id":58,"pagetitle":"Reference","title":"Types","ref":"/DataToolkitDocs/ref/stable/reference/#Types-2","content":" Types"},{"id":59,"pagetitle":"Reference","title":"DataToolkitBase","ref":"/DataToolkitDocs/ref/stable/reference/#DataToolkitBase-5","content":" DataToolkitBase Missing docstring. Missing docstring for  DataToolkit.DataCollection . Check Documenter's build log for details. Missing docstring. Missing docstring for  DataToolkit.DataSet . Check Documenter's build log for details. Missing docstring. Missing docstring for  DataToolkit.DataStorage . Check Documenter's build log for details. Missing docstring. Missing docstring for  DataToolkit.DataLoader . Check Documenter's build log for details. Missing docstring. Missing docstring for  DataToolkit.DataWriter . Check Documenter's build log for details. Missing docstring. Missing docstring for  DataToolkit.Identifier . Check Documenter's build log for details. Missing docstring. Missing docstring for  DataToolkit.Plugin . Check Documenter's build log for details."},{"id":64,"pagetitle":"Tutorial","title":"Tutorial","ref":"/DataToolkitDocs/ref/stable/tutorial/#Tutorial","content":" Tutorial In this tutorial you will be guided through some of the main usage patterns involving  DataToolkit . After doing the first step ( Initialising a Data Collection ), all other sections can be treated as self-contained exercises."},{"id":65,"pagetitle":"Tutorial","title":"Initialising a Data Collection","ref":"/DataToolkitDocs/ref/stable/tutorial/#Initialising-a-Data-Collection","content":" Initialising a Data Collection First, we will create a new environment to run through the tutorial in, and load the  DataToolkit  package. julia> using Pkg\n\njulia> expanduser(\"~/Documents/datatoolkit_tutorial\") |> mkpath |> cd\n\njulia> Pkg.activate(\".\")\n  Activating new project at `~/Documents/datatoolkit_tutorial`\n\njulia> Pkg.add(\"DataToolkit\")\n   Resolving package versions...\n    Updating `~/Documents/datatoolkit_tutorial/Project.toml`\n  [dc83c90b] + DataToolkit\n  ...\nPrecompiling project...\n\njulia> using DataToolkit Notice that by typing  }  at an empty  julia>  prompt, the REPL prompt will change to  (⋅) data>  (in the same way that typing  ]  enters the  pkg>  REPL). This is the \"Data REPL\", and the  (⋅)  prefix indicates the current project. When there is no current project, the dot is shown. In the data REPL, we can see a list of all the available commands by typing  help  or  ? , which will pull up a command list like so: (⋅) data> help\n Command  Action\n ────────────────────────────────────────────────────────\n <cmd>    <brief description of what cmd does>\n ...      ...\n help     Display help text for commands and transformers Tip Get more information on a particular command with  help <cmd> , you can even get more information on what  help  does with  help help  😉. We will initialise a new data collection with the  init  command. Note We can use the full command ( init ), or any substring that uniquely identifies the command (e.g.  it ). (⋅) data> init\n Create Data.toml for current project? [Y/n]: y\n Name: tutorial\n Use checksums by default? [Y/n]: n\n ✓ Created new data collection 'tutorial' at /home/tec/Documents/datatoolkit_tutorial/Data.toml Tip There are a few other ways  init  can be used, see the full docs with  help init . If we look at the  ~/Documents/datatoolkit_tutorial  folder, we should now see three files. shell> tree\n.\n├── Data.toml\n├── Manifest.toml\n└── Project.toml Looking inside the  Data.toml , we can see what a data collection with no data sets looks like: data_config_version = 0\nuuid = \"f20a77d0-0dc9-41bb-875b-ad0bf42c90bd\"\nname = \"tutorial\"\nplugins = [\"store\", \"defaults\", \"memorise\"] Note The plugins  store ,  defaults , and  memorise  are the default set of plugins, which is why we see them here. A minimal  Data.toml  would have  plugins = [] . At this point, we have created a new data collection, and seen what is created. If we close the Julia session and re-open a REPL in the  datatoolkit_test  project, loading  DataToolkit  will automatically cause the data collection we just created to be loaded as well, as seen in the prompt prefix. julia> using DataToolkit\n\n(tutorial) data> # after typing '}'"},{"id":66,"pagetitle":"Tutorial","title":"Adding and loading the Iris data set","ref":"/DataToolkitDocs/ref/stable/tutorial/#Adding-and-loading-the-Iris-data-set","content":" Adding and loading the Iris data set Now we have a data collection, we can add data sets too it. Fisher's  Iris  data set is part of the  scikit-learn  repository, which makes it fairly easy to find a link to it:  https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv We can easily add this as a  DataSet  using the  add  Data REPL command, (tutorial) data> add iris https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\n Description: Fisher's famous Iris flower measurements\n ✓ Created 'iris' (3f3d7714-22aa-4555-a950-78f43b74b81c)\n DataSet tutorial:iris\n  Storage: web(IO, Vector{UInt8}, String, FilePath)\n  Loaders: csv(DataFrame, Matrix, File) Note Say halfway through we decide we don't want to proceed with this Data REPL command, at any point we can interrupt it with  ^C  (Control +  C ) and abort the action. This works with other Data REPL commands in the same way. (tutorial) data> add iris https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\n Description:  ! Aborted The  add  command tries to be a bit clever and guess how the data should be acquired and loaded. In this case it (correctly) guessed that this file should be downloaded from the web, and loaded as a CSV. It is worth noting that downloading will occur when  iris  is first accessed or the  store fetch  Data REPL command is run. The  DataSet tutorial:iris  and  Storage: ​/​ Loaders:  lines are how all  DataSet ​s are displayed. Using the  dataset  function we can obtain any data set easily by name, and so  dataset(\"iris\")  will show the same information. julia> dataset(\"iris\")\nDataSet tutorial:iris\n  Storage: web(IO, Vector{UInt8}, String, FilePath)\n  Loaders: csv(DataFrame, Matrix, File) We can see from the  Storage: web(IO, Vector{UInt8}, String, FilePath)  line that the  web  storage driver is being used, and it can make the content available as an  IO ,  Vector{UInt8} ,  String , or  FilePath  (a string wrapper type provided by  DataToolkitBase  for dispatch purposes). Similarly, the  Loaders: csv(DataFrame, Matrix, File)  tells us that the  csv  loader is being used, and it can provide a  DataFrame ,  Matrix , or  CSV.File . If we look at the  Data.toml  again, we can see how the  iris  data set is represented: [[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\" To obtain a particular loaded form of the data set, we can use the  read  function. For instance,  read(dataset(\"iris\"), DataFrame)  or  read(dataset(\"iris\"), Matrix) . We can also omit the second argument, in which case the first form that  can  be loaded will be (e.g. in this case since  DataFrames  is not loaded,  iris  can not be loaded as a  DataFrame , but it can be loaded as a  Matrix , and so it will be). julia> read(dataset(\"iris\"))\n[ Info: Lazy-loading CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n │ Package CSV not found, but a package named CSV is available from a registry.\n │ Install package?\n │   (datatoolkit_tutorial) pkg> add CSV\n └ (y/n/o) [y]:\n   Resolving package versions...\n    Updating `~/Documents/datatoolkit_tutorial/Project.toml`\n  [336ed68f] + CSV v0.10.11\n    Updating `~/Documents/datatoolkit_tutorial/Manifest.toml`\nPrecompiling project...\n[ Info: Lazy-loading CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n150×5 Matrix{Float64}:\n 5.1  3.5  1.4  0.2  0.0\n 4.9  3.0  1.4  0.2  0.0\n ...\n 6.2  3.4  5.4  2.3  2.0\n 5.9  3.0  5.1  1.8  2.0 Note We haven't installed the  CSV  package, but thanks to the lazy-loading system we are presented with the option to install it on-the-fly. Of course, you can always just load the  CSV  package yourself trying to access information that uses the  csv  loader. Because  read(dataset(\"iris\"))  is a fairly common pattern, for convenience there is a  d\"\"  \"data set in loaded form\" macro.  d\"iris\"  is equivalent to  read(dataset(\"iris\")) . Having the  iris  data as a  Matrix  is fine, but it would be nicer to have it as a  DataFrame . Since that is the first format listed, if we just install  DataFrames  and ask for  iris  again (but this time using the  d\"\"  macro). julia> using DataFrames\n │ Package DataFrames not found, but a package named DataFrames is available from a registry.\n │ Install package?\n │   (datatoolkit_tutorial) pkg> add DataFrames\n └ (y/n/o) [y]:\n   Resolving package versions...\n    Updating `~/Documents/datatoolkit_tutorial/Project.toml`\n  [a93c6f00] + DataFrames v1.5.0\n  ...\n  1 dependency successfully precompiled in 25 seconds. 41 already precompiled.\n\njulia> d\"iris\"\n150×5 DataFrame\n Row │ 150      4        setosa   versicolor  virginica\n     │ Float64  Float64  Float64  Float64     Int64\n─────┼──────────────────────────────────────────────────\n   1 │     5.1      3.5      1.4         0.2          0\n   2 │     4.9      3.0      1.4         0.2          0\n   3 │     4.7      3.2      1.3         0.2          0\n  ⋮  │    ⋮        ⋮        ⋮         ⋮           ⋮\n 149 │     6.2      3.4      5.4         2.3          2\n 150 │     5.9      3.0      5.1         1.8          2 That's nicer, but wait, those column names aren't right! The first line appears to be describing the size of the data (150×4) and the three category names, when the columns should be: sepal_length , sepal_width , petal_length , petal_width , and species_class Perhaps there's a way we can specify the correct column names? We could check the online docs for the CSV loader, but we can also look at them with the  help  Data REPL command. (tutorial) data> help :csv\n  Parse and serialize CSV data\n\n  ...\n\n  Parameters\n  ≡≡≡≡≡≡≡≡≡≡≡≡\n\n    •  args: keyword arguments to be provided to CSV.File, see\n       https://csv.juliadata.org/stable/reading.html#CSV.File.\n\n  As a quick-reference, some arguments of particular interest are:\n\n    •  header: Either,\n       • the row number to parse for column names\n       • the list of column names\n\n  ... Perfect! Looks like we can just set the  args.header  parameter of the  csv  loader, and we'll get the right column names. To easily do so, we can make use of the  edit  Data REPL command, which opens up a TOML file with just a single data set in  $JULIA_EDITOR  (which defaults to  $VISUAL ​/​ $EDITOR ) and records the changes upon exit. (tutorial) data> edit iris Setting  args.header  is as simple as editing the  iris  loader to the following value (adding one line): [[iris.loader]]\ndriver = \"csv\"\nargs.header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"] After saving and exiting, you'll be presented with a summary of the changes and a prompt to accept them. (tutorial) data> edit iris\n ~ Modified loader:\n   ~ Modified [1]:\n     + Added args\n Does this look correct? [y/N]: y\n ✓ Edited 'iris' (3f3d7714-22aa-4555-a950-78f43b74b81c) Now if we ask for the  iris  data set again, we should see the correct headers. julia> d\"iris\"\n151×5 DataFrame\n Row │ sepal_length  sepal_width  petal_length  petal_width  species_class\n     │ Float64       Float64      String7       String15     String15\n─────┼─────────────────────────────────────────────────────────────────────\n   1 │        150.0          4.0  setosa        versicolor   virginica\n   2 │          5.1          3.5  1.4           0.2          0\n   3 │          4.9          3.0  1.4           0.2          0\n  ⋮  │      ⋮             ⋮            ⋮             ⋮             ⋮\n 150 │          6.2          3.4  5.4           2.3          2\n 151 │          5.9          3.0  5.1           1.8          2 The headers are correct, but now the first line is counted as part of the data. This can be fixed by editing  iris  again and setting  args.skipto  to  2  in the  csv  loader settings. The final  iris  entry in the  Data.toml  should look like so: [[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\"\n\n        [iris.loader.args]\n        header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"]\n        skipto = 2 Now, you have a  Project.toml ,  Manifest.toml , and  Data.toml  that can be relocated to other systems and  d\"iris\"  will consistently produce the exact same  DataFrame ."},{"id":67,"pagetitle":"Tutorial","title":"Bonus: ensuring the integrity of the downloaded data","ref":"/DataToolkitDocs/ref/stable/tutorial/#Bonus:-ensuring-the-integrity-of-the-downloaded-data","content":" Bonus: ensuring the integrity of the downloaded data One of the three plugins used by default is the  store  plugin. It is responsible for caching IO data and checking data validity. For a more complete description of what it does, see the web docs or the Data REPL (sub)command  plugin info store . There are two immediate impacts of this plugin we can easily observe. The first is that we can load the  iris  data set offline in a fresh Julia session, and in fact if we copy the  iris  specification into a separate data set it will re-use the  same  downloaded data. The second, is that by setting  iris 's  web  storage driver's  checksum  property to  \"auto\" , the next time we load  iris  a checksum will be generated and saved. If in future the  web  storage driver produces different data, this will now be caught and raised. This can be done automatically by setting the default value to  \"auto\" , which we were prompted to do during initialisation."},{"id":68,"pagetitle":"Tutorial","title":"Multi-step analysis with the Boston Housing data set","ref":"/DataToolkitDocs/ref/stable/tutorial/#Multi-step-analysis-with-the-Boston-Housing-data-set","content":" Multi-step analysis with the Boston Housing data set"},{"id":69,"pagetitle":"Tutorial","title":"Loading the data","ref":"/DataToolkitDocs/ref/stable/tutorial/#Loading-the-data","content":" Loading the data The  Boston Housing  data set is part of the  RDatasets  package, and we can obtain a link to the raw data file in the repository:  https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz As with the Iris data, we will use the  add  Data REPL command to conveniently create a new data set. (tutorial) data> add boston https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz\n Description: The Boston Housing data set. This contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.\n ✓ Created 'boston' (02968c42-828e-4f22-86b8-ec67ac629a03)\n DataSet tutorial:boston\n  Storage: web(IO, Vector{UInt8}, String, FilePath)\n  Loaders: chain(DataFrame, Matrix, File) This example is a bit more complicated because we have a gzipped CSV. There is a gzip-decompressing loader, and a CSV loader, but no single loader that does both. Thankfully, there is a special loader called  chain  that allows for multiple loaders to be  chained  together. We can see it's automatically been used here, and if we inspect the  Data.toml  we an see the following generated representation of the boston housing data, in which the  gzip  and  csv  loaders are both used. [[boston]]\nuuid = \"02968c42-828e-4f22-86b8-ec67ac629a03\"\ndescription = \"The Boston Housing data set. This contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.\"\n\n    [[boston.storage]]\n    driver = \"web\"\n    url = \"https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz\"\n\n    [[boston.loader]]\n    driver = \"chain\"\n    loaders = [\"gzip\", \"csv\"]\n    type = [\"DataFrame\", \"Matrix\", \"CSV.File\"] Note We can see the loaders  chain  passes the  data  through are given by  loaders = [\"gzip\", \"csv\"] . For more information on the  chain  loader see  help :chain  in the Data REPL or the online documentation. Thanks to this cleverness, obtaining the Boston Housing data as a nice  DataFrame  is as simple as  d\"boston\"  (when  DataFrames  is loaded). julia> d\"boston\"\n506×14 DataFrame\n Row │ Crim     Zn       Indus    Chas   NOx      Rm       Age      Dis      Rad   ⋯\n     │ Float64  Float64  Float64  Int64  Float64  Float64  Float64  Float64  Int64 ⋯\n─────┼──────────────────────────────────────────────────────────────────────────────\n   1 │ 0.00632     18.0     2.31      0    0.538    6.575     65.2   4.09        1 ⋯\n   2 │ 0.02731      0.0     7.07      0    0.469    6.421     78.9   4.9671      2\n   3 │ 0.02729      0.0     7.07      0    0.469    7.185     61.1   4.9671      2\n  ⋮  │    ⋮        ⋮        ⋮       ⋮       ⋮        ⋮        ⋮        ⋮       ⋮   ⋱\n 504 │ 0.06076      0.0    11.93      0    0.573    6.976     91.0   2.1675      1\n 505 │ 0.10959      0.0    11.93      0    0.573    6.794     89.3   2.3889      1\n 506 │ 0.04741      0.0    11.93      0    0.573    6.03      80.8   2.505       1 ⋯"},{"id":70,"pagetitle":"Tutorial","title":"Cleaning the data","ref":"/DataToolkitDocs/ref/stable/tutorial/#Cleaning-the-data","content":" Cleaning the data Say the data needs some massaging, such as imputation, outlier removal, or restructuring. We can cleanly handle this by creating a  second  dataset that uses the value of the  initial  dataset. Say we consider this initial data unclean, and that to \"clean\" this dataset we filter out the entries where the we only keep entries where the  MedV  value is within the 90% quantile. We can easily do this with the  make  Data REPL command. For this, we'll want to use the  StatsBase  package, so we'll add it and then make it available to use with  DataToolkit.@addpkgs . (datatoolkit_tutorial) pkg> add StatsBase\n\njulia> DataToolkit.@addpkgs StatsBase Info The  DataToolkit.@addpkgs StatsBase  line will need to be executed in every fresh Julia session, when creating a data  package  it makes sense to put this within the  __init__  function. Now we can create the  boston (clean)  dataset with the  make  command. (tutorial) data> make boston (clean)\n\n(data) julia> @import StatsBase.quantile\nstd (generic function with 5 methods)\n\n(data) julia> proportion = 0.8\n0.8\n\n(data) julia> column = \"MedV\"\n\n(data) julia> vals = d\"boston\"[!, column]\n506-element Vector{Float64}:\n\n(data) julia> minval, maxval = quantile(vals, [0.5 - proportion/2, 0.5 + proportion/2])\n2-element Vector{Float64}:\n 10.2\n 43.4\n\n(data) julia> mask = minval .<= vals .<= maxval\n506-element BitVector:\n\n(data) julia> d\"boston\"[mask, :]\n456×14 DataFrame...\n\n^D\n\n Would you like to edit the final script? [Y/n]: n\n What is the type of the returned value? DataFrame\n Description: Cleaned Boston Housing data\n Should the script be inserted inline (i), or as a file (f)? i\n ✓ Created 'boston (clean)' (5162814a-120f-4cdc-9958-620189295330)\n\n(tutorial) data> We can look inside the  Data.toml  to see the new entry. [[\"boston (clean)\"]]\nuuid = \"5162814a-120f-4cdc-9958-620189295330\"\ndescription = \"Cleaned Boston Housing data\"\n\n    [[\"boston (clean)\".loader]]\n    driver = \"julia\"\n    function = '''\nfunction (; var\"data#boston\")\n    @import StatsBase.quantile\n    proportion = 0.8\n    column = \"MedV\"\n    vals = var\"data#boston\"[!, column]\n    (minval, maxval) = quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\n    mask = minval .<= vals .<= maxval\n    var\"data#boston\"[mask, :]\nend\n'''\n    type = \"DataFrame\"\n\n        [\"boston (clean)\".loader.arguments]\n        \"data#boston\" = \"📇DATASET<<boston::DataFrame>>\""},{"id":71,"pagetitle":"Tutorial","title":"Fitting a linear model","ref":"/DataToolkitDocs/ref/stable/tutorial/#Fitting-a-linear-model","content":" Fitting a linear model Now let's say we want to fit a linear model for the relationship between  MedV  and  Rm . We could do this in a script … or create another derived dataset. Let's do this with  GLM , so first run  ] add GLM , then  DataToolkit.@addpkgs GLM . Now we'll create another derived data set with  make . (tutorial) data> make boston Rm ~ MedV\n\n(data) julia> @import GLM: @formula, lm\n\n(data) julia> lm(@formula(Rm ~ MedV), d\"boston (clean)\")\n\n^D\n\n Would you like to edit the final script? [Y/n]: n\n What is the type of the returned value? Any\n Description: A linear model for the relation between Rm and MedV\n Should the script be inserted inline (i), or as a file (f)? i\n ✓ Created 'boston Rm ~ MedV' (e720acb2-5ed1-417f-bfd0-668c21134c87)\n\n(tutorial) data> Info For now, manually specify  Any  as the return type instead of the default  StatsModels.TableRegressionModel{GLM.LinearModel{GLM.LmResp{Array{Float64,1}},GLM.DensePredChol{Float64,LinearAlgebra.CholeskyPivoted{Float64,Array{Float64,2},Array{Int64,1}}}},Array{Float64,2}} . It's currently difficult for  DataToolkitBase  to represent types that rely on nested modules, which occurs here. Obtaining the linear regression result is as easy as fetching any other dataset. julia> d\"boston Rm ~ MedV\"\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nRm ~ 1 + MedV\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  4.95241     0.0753342  65.74    <1e-99  4.80436    5.10045\nMedV         0.0588453   0.0033047  17.81    <1e-53  0.0523509  0.0653397\n─────────────────────────────────────────────────────────────────────────"},{"id":72,"pagetitle":"Tutorial","title":"A more easily tunable cleaner","ref":"/DataToolkitDocs/ref/stable/tutorial/#A-more-easily-tunable-cleaner","content":" A more easily tunable cleaner In the current implementation of  boston (clean) , we hardcoded a  proportion  value of  0.8 , and set the  column  to  \"MedV\" . It could be nice if we made those more easily tunable. We can do this by turning them into keyword arguments of the function. To make this change, we will use the  edit  Data REPL command. (tutorial) data> edit boston (clean) This will open up a temporary TOML file containing the  boston (clean)  dataset in your text editor of choice. In this file, change the function to: function (; var\"data#boston\", proportion, column)\n    @import StatsBase.quantile\n    vals = var\"data#boston\"[!, column]\n    (minval, maxval) = quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\n    mask = minval .<= vals .<= maxval\n    var\"data#boston\"[mask, :]\nend We will then move the  proportion = 0.8  and  column = \"MedV\"  lines to the arguments table. [\"boston (clean)\".loader.arguments]\n\"data#boston\" = \"📇DATASET<<boston::DataFrame>>\"\nproportion = 0.8\ncolumn = \"MedV\" Aftre making these changes and closing the file, we'll be asked if we want to make this change (we do). (tutorial) data> edit boston (clean)\n ~ Modified loader:\n   ~ Modified [1]:\n     ~ Modified arguments:\n       + Added column\n       + Added proportion\n     ~ Modified function:\n       \"function (; var\\\"data#boston\\\")\\n    @import StatsBase.quantile\\n    proportion = 0.9\\n    column = \\\"MedV\\\"\\n    vals = var\\\"data#boston\\\"[!, column]\\n    (minval, maxval) = quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\\n    mask = minval .<= vals .<= maxval\\n    var\\\"data#boston\\\"[mask, :]\\nend\\n\" ~> \"function (; var\\\"data#boston\\\", proportion, column)\\n    @import StatsBase.quantile\\n    vals = var\\\"data#boston\\\"[!, column]\\n    (minval, maxval) = quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\\n    mask = minval .<= vals .<= maxval\\n    var\\\"data#boston\\\"[mask, :]\\nend\\n\"\n Does this look correct? [y/N]: y\n ✓ Edited 'boston (clean)' (5162814a-120f-4cdc-9958-620189295330)"},{"id":73,"pagetitle":"Tutorial","title":"Propagating changes","ref":"/DataToolkitDocs/ref/stable/tutorial/#Propagating-changes","content":" Propagating changes With our new parameterisation of the cleaning step, we can now easily tune the cleaning step. We can see the results of this propagating through in the  boston Rm ~ MedV  dataset. First, see that the  d\"boston Rm ~ MedV\"  result is the same as it was before. julia> d\"boston Rm ~ MedV\"\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nRm ~ 1 + MedV\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  4.98609     0.0978742  50.94    <1e-99  4.79369    5.1785\nMedV         0.0563562   0.0044222  12.74    <1e-30  0.0476627  0.0650497\n───────────────────────────────────────────────────────────────────────── Now,  edit  the  boston (clean)  dataset again and change the  proportion  to  0.95 . (tutorial) data> edit boston (clean)\n ~ Modified loader:\n   ~ Modified [1]:\n     ~ Modified arguments:\n       ~ Modified proportion:\n         0.8 ~> 0.95\n Does this look correct? [y/N]: y\n ✓ Edited 'boston (clean)' (5162814a-120f-4cdc-9958-620189295330) Since  boston (clean)  is an input of  boston Rm ~ MedV , and all inputs are recursively hashed (like in a  Merkle tree ), we can immediately see the (small) change simply by fetching it again — it is automatically recomputed. julia> d\"boston Rm ~ MedV\"\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nRm ~ 1 + MedV\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  5.05849    0.0618848   81.74    <1e-99   4.9369    5.18008\nMedV         0.0541727  0.00251511  21.54    <1e-72   0.049231  0.0591144\n─────────────────────────────────────────────────────────────────────────"},{"id":74,"pagetitle":"Tutorial","title":"The final  Data.toml","ref":"/DataToolkitDocs/ref/stable/tutorial/#The-final-Data.toml","content":" The final  Data.toml At the end of this tutorial (or should you wish to just poke at the results), you should end up with a  Data.toml  that looks like this: data_config_version = 0\nuuid = \"f20a77d0-0dc9-41bb-875b-ad0bf42c90bd\"\nname = \"tutorial\"\nplugins = [\"defaults\", \"store\"]\n\n[[boston]]\nuuid = \"02968c42-828e-4f22-86b8-ec67ac629a03\"\ndescription = \"The Boston Housing data set. This contains information collected by the U.S Census Service concerning housing in the area of Boston Mass.\"\n\n    [[boston.storage]]\n    driver = \"web\"\n    url = \"https://github.com/JuliaStats/RDatasets.jl/raw/v0.7.0/data/MASS/Boston.csv.gz\"\n\n    [[boston.loader]]\n    driver = \"chain\"\n    loaders = [\"gzip\", \"csv\"]\n\n[[\"boston (clean)\"]]\nuuid = \"5162814a-120f-4cdc-9958-620189295330\"\ndescription = \"Cleaned Boston Housing data\"\n\n    [[\"boston (clean)\".loader]]\n    driver = \"julia\"\n    function = '''\nfunction (; var\"data#boston\", proportion, column)\n    @import StatsBase.quantile\n    vals = var\"data#boston\"[!, column]\n    (minval, maxval) = quantile(vals, [0.5 - proportion / 2, 0.5 + proportion / 2])\n    mask = minval .<= vals .<= maxval\n    var\"data#boston\"[mask, :]\nend\n'''\n    type = \"DataFrame\"\n\n        [\"boston (clean)\".loader.arguments]\n        column = \"MedV\"\n        \"data#boston\" = \"📇DATASET<<boston::DataFrame>>\"\n        proportion = 0.95\n\n[[\"boston Rm ~ MedV\"]]\nuuid = \"e720acb2-5ed1-417f-bfd0-668c21134c87\"\ndescription = \"A linear model for the relation between Rm and MedV\"\n\n    [[\"boston Rm ~ MedV\".loader]]\n    driver = \"julia\"\n    function = \"\"\"\nfunction (; var\\\"data#boston (clean)\\\")\n    @import GLM\n    GLM.lm(GLM.@formula(Rm ~ MedV), var\\\"data#boston (clean)\\\")\nend\n\"\"\"\n\n        [\"boston Rm ~ MedV\".loader.arguments]\n        \"data#boston (clean)\" = \"📇DATASET<<boston (clean)::DataFrame>>\"\n\n[[iris]]\nuuid = \"3f3d7714-22aa-4555-a950-78f43b74b81c\"\ndescription = \"Fisher's famous Iris flower measurements\"\n\n    [[iris.storage]]\n    driver = \"web\"\n    checksum = \"crc32c:d5c06b86\"\n    url = \"https://raw.githubusercontent.com/scikit-learn/scikit-learn/1.0/sklearn/datasets/data/iris.csv\"\n\n    [[iris.loader]]\n    driver = \"csv\"\n\n        [iris.loader.args]\n        header = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species_class\"]\n        skipto = 2"},{"id":77,"pagetitle":"Introduction","title":"Introduction","ref":"/DataToolkitDocs/base/stable/#Introduction","content":" Introduction"},{"id":78,"pagetitle":"Introduction","title":"The problem with the current state of affairs","ref":"/DataToolkitDocs/base/stable/#The-problem-with-the-current-state-of-affairs","content":" The problem with the current state of affairs Data is beguiling. It can initially seem simple to deal with: \"here I have a file, and that's it\". However as soon as you do things with the data you're prone to be asked tricky questions like: where's the data? how did you process that data? how can I be sure I'm looking at the same data as you? This is no small part of the  replication crisis . Further concerns arise as soon as you start dealing with large quantities of data, or computationally expensive derived data sets. For example: Have I already computed this data set somewhere else? Is my generated data up to date with its sources/dependencies? Generic tools exist for many parts of this problem, but there are some benefits that can be realised by creating a Julia-specific system, namely: Having all pertinent environmental information in the data processing contained in a single  Project.toml Improved convenience in data loading and management, compared to a generic solution Allowing datasets to be easily shared with a Julia package In addition, the Julia community seems to have a strong tendency to NIH [NIH]  tools, so we may as well get ahead of this and try to make something good 😛."},{"id":79,"pagetitle":"Introduction","title":"Pre-existing solutions","ref":"/DataToolkitDocs/base/stable/#Pre-existing-solutions","content":" Pre-existing solutions"},{"id":80,"pagetitle":"Introduction","title":"DataLad","ref":"/DataToolkitDocs/base/stable/#DataLad","content":" DataLad Does a lot of things well Puts information on how to create data in git commit messages (bad) No data file specification"},{"id":81,"pagetitle":"Introduction","title":"Kedro data catalog","ref":"/DataToolkitDocs/base/stable/#Kedro-data-catalog","content":" Kedro data catalog Has a file defining all the data (good) Has poor versioning https://kedro.readthedocs.io/en/stable/data/data_catalog.html Data Catalog CLI"},{"id":82,"pagetitle":"Introduction","title":"Snakemake","ref":"/DataToolkitDocs/base/stable/#Snakemake","content":" Snakemake Workflow manager, with remote file support Snakemake Remote Files Good list of possible file locations to handle Drawback is that you have to specify the location you expect(S3, http, FTP, etc.) No data file specification"},{"id":83,"pagetitle":"Introduction","title":"Nextflow","ref":"/DataToolkitDocs/base/stable/#Nextflow","content":" Nextflow Workflow manager, with remote file support Docs on files and IO Docs on S3 You just call  file()  and nextflow figures out under the hood the protocol whether it should pull it from S3, http, FTP, or a local file. No data file specification NIH Not Invented Here, a tendency to \"reinvent the wheel\" to avoid using tools from external origins &mdash; it would of course be better if you (re)made it."},{"id":86,"pagetitle":"Data Advice","title":"Data Advising","ref":"/DataToolkitDocs/base/stable/advising/#Data-Advising","content":" Data Advising"},{"id":87,"pagetitle":"Data Advice","title":"Advice","ref":"/DataToolkitDocs/base/stable/advising/#Advice","content":" Advice"},{"id":88,"pagetitle":"Data Advice","title":"DataToolkitBase.Advice","ref":"/DataToolkitDocs/base/stable/advising/#DataToolkitBase.Advice","content":" DataToolkitBase.Advice  —  Type Advice{func, context} <: Function Advices allow for composable, highly flexible modifications of data by encapsulating a function call. They are inspired by elisp's advice system, namely the most versatile form —  :around  advice, and Clojure's advisors. A  Advice  is essentially a function wrapper, with a  priority::Int  attribute. The wrapped functions should be of the form: (action::Function, args...; kargs...) ->\n  ([post::Function], action::Function, args::Tuple, [kargs::NamedTuple]) Short-hand return values with  post  or  kargs  omitted are also accepted, in which case default values (the  identity  function and  (;)  respectively) will be automatically substituted in.     input=(action args kwargs)\n         ┃                 ┏╸post=identity\n       ╭─╂────advisor 1────╂─╮\n       ╰─╂─────────────────╂─╯\n       ╭─╂────advisor 2────╂─╮\n       ╰─╂─────────────────╂─╯\n       ╭─╂────advisor 3────╂─╮\n       ╰─╂─────────────────╂─╯\n         ┃                 ┃\n         ▼                 ▽\naction(args; kargs) ━━━━▶ post╺━━▶ result To specify which transforms a Advice should be applied to, ensure you add the relevant type parameters to your transducing function. In cases where the transducing function is not applicable, the Advice will simply act as the identity function. After all applicable  Advice s have been applied,  action(args...; kargs...) |> post  is called to produce the final result. The final  post  function is created by rightwards-composition with every  post  entry of the advice forms (i.e. at each stage  post = post ∘ extra  is run). The overall behaviour can be thought of as  shells  of advice.         ╭╌ advisor 1 ╌╌╌╌╌╌╌╌─╮\n        ┆ ╭╌ advisor 2 ╌╌╌╌╌╮ ┆\n        ┆ ┆                 ┆ ┆\ninput ━━┿━┿━━━▶ function ━━━┿━┿━━▶ result\n        ┆ ╰╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╯ ┆\n        ╰╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╯ Constructors Advice(priority::Int, f::Function)\nAdvice(f::Function) # priority is set to 1 Examples 1. Logging every time a DataSet is loaded. loggingadvisor = Advice(\n    function(post::Function, f::typeof(load), loader::DataLoader, input, outtype)\n        @info \"Loading $(loader.data.name)\"\n        (post, f, (loader, input, outtype))\n    end) 2. Automatically committing each data file write. writecommitadvisor = Advice(\n    function(post::Function, f::typeof(write), writer::DataWriter{:filesystem}, output, info)\n        function writecommit(result)\n            run(`git add $output`)\n            run(`git commit -m \"update $output\"`)\n            result\n        end\n        (post ∘ writecommit, writefn, (output, info))\n    end) source"},{"id":89,"pagetitle":"Data Advice","title":"Advisement points","ref":"/DataToolkitDocs/base/stable/advising/#Advisement-points","content":" Advisement points"},{"id":90,"pagetitle":"Data Advice","title":"Parsing and serialisation of data sets and collections","ref":"/DataToolkitDocs/base/stable/advising/#Parsing-and-serialisation-of-data-sets-and-collections","content":" Parsing and serialisation of data sets and collections DataCollection ​s,  DataSet ​s, and  AbstractDataTransformer ​s are advised at two stages during parsing: When calling  fromspec  on the  Dict  representation, at the start of parsing At the end of the  fromspec  function, calling  identity  on the object Serialisation is performed through the  tospec  call, which is also advised. The signatures of the advised function calls are as follows: fromspec(DataCollection, spec::Dict{String, Any}; path::Union{String, Nothing})::DataCollection\nidentity(collection::DataCollection)::DataCollection\ntospec(collection::DataCollection)::Dict fromspec(DataSet, collection::DataCollection, name::String, spec::Dict{String, Any})::DataSet\nidentity(dataset::DataSet)::DataSet\ntospec(dataset::DataSet)::Dict fromspec(ADT::Type{<:AbstractDataTransformer}, dataset::DataSet, spec::Dict{String, Any})::ADT\nidentity(adt::AbstractDataTransformer)::AbstractDataTransformer\ntospec(adt::AbstractDataTransformer)::Dict"},{"id":91,"pagetitle":"Data Advice","title":"Processing identifiers","ref":"/DataToolkitDocs/base/stable/advising/#Processing-identifiers","content":" Processing identifiers Both the parsing of an  Identifier  from a string, and the serialisation of an  Identifier  to a string are advised. Specifically, the following function calls: parse_ident(spec::AbstractString)\nstring(ident::Identifier)"},{"id":92,"pagetitle":"Data Advice","title":"The data flow arrows","ref":"/DataToolkitDocs/base/stable/advising/#The-data-flow-arrows","content":" The data flow arrows The reading, writing, and storage of data may all be advised. Specifically, the following function calls: load(loader::DataLoader, datahandle, as::Type)\nstorage(provider::DataStorage, as::Type; write::Bool)\nsave(writer::DataWriter, datahandle, info)"},{"id":93,"pagetitle":"Data Advice","title":"Index of advised calls","ref":"/DataToolkitDocs/base/stable/advising/#Index-of-advised-calls","content":" Index of advised calls There are  33  advised function calls, across  9  files, covering  12  functions (automatically detected). Arranged by function _read  (2 instances) externals.jl On line 151  _read(dataset, as)  is advised within a  Base.read  method. On line 160  _read(dataset, as)  is advised within a  Base.read  method. fromspec  (5 instances) manipulation.jl On line 435  fromspec(DataSet, collection, name, spec)  is advised within a  add  method. On line 572  fromspec(T, dataset, process_spec(spec, drv))  is advised within a  create  method. On line 579  fromspec(T, dataset, process_spec(spec, driver))  is advised within a  create  method. parser.jl On line 118  fromspec(ADT, dataset, spec)  is advised within a  ADT::Type{<:AbstractDataTransformer}  method. On line 253  fromspec(DataSet, collection, name, spec)  is advised within a  DataSet  method. identity  (3 instances) parser.jl On line 163  identity(ADT(dataset, ttype, priority, dataset_parameters(dataset, Val(:extract), parameters)))  is advised within a  fromspec  method. On line 245  identity(collection)  is advised within a  fromspec  method. On line 283  identity(dataset)  is advised within a  fromspec  method. init  (1 instance) manipulation.jl On line 53  init(newcollection)  is advised within a  init  method. lint  (1 instance) lint.jl On line 84  lint(obj, linters)  is advised within a  lint(obj::T)  method. load  (2 instances) externals.jl On line 220  load(loader, datahandle, as)  is advised within a  _read  method. On line 233  load(loader, nothing, as)  is advised within a  _read  method. parse_ident  (8 instances) externals.jl On line 80  parse_ident(identstr)  is advised within a  dataset  method. On line 84  parse_ident(identstr)  is advised within a  dataset  method. errors.jl On line 44  parse_ident(err.identifier)  is advised within a  Base.showerror  method. On line 53  parse_ident(err.identifier)  is advised within a  Base.showerror  method. identification.jl On line 190  parse_ident(identstr)  is advised within a  resolve  method. On line 195  parse_ident(identstr)  is advised within a  resolve  method. parameters.jl On line 40  parse_ident(dsid_match.captures[1])  is advised within a  dataset_parameters  method. parser.jl On line 72  parse_ident(spec)  is advised within a  Base.parse  method. refine  (1 instance) identification.jl On line 146  refine(matchingdatasets, ident, String[])  is advised within a  refine  method. save  (1 instance) externals.jl On line 376  save(writer, datahandle, info)  is advised within a  Base.write(dataset::DataSet, info::T)  method. storage  (1 instance) externals.jl On line 311  storage(storage_provider, as; write)  is advised within a  Base.open  method. string  (5 instances) display.jl On line 74  string(nameonly)  is advised within a  Base.show  method. errors.jl On line 82  string(ident)  is advised within a  Base.showerror  method. On line 90  string(ident)  is advised within a  Base.showerror  method. identification.jl On line 111  string(ident)  is advised within a  resolve  method. parameters.jl On line 52  string(param)  is advised within a  dataset_parameters  method. tospec  (3 instances) writer.jl On line 51  tospec(adt)  is advised within a  Base.convert  method. On line 73  tospec(ds)  is advised within a  Base.convert  method. On line 87  tospec(dc)  is advised within a  Base.convert  method. Arranged by file display.jl  (1 instance) On line 74  string(nameonly)  is advised within a  Base.show  method. externals.jl  (8 instances) On line 80  parse_ident(identstr)  is advised within a  dataset  method. On line 84  parse_ident(identstr)  is advised within a  dataset  method. On line 151  _read(dataset, as)  is advised within a  Base.read  method. On line 160  _read(dataset, as)  is advised within a  Base.read  method. On line 220  load(loader, datahandle, as)  is advised within a  _read  method. On line 233  load(loader, nothing, as)  is advised within a  _read  method. On line 311  storage(storage_provider, as; write)  is advised within a  Base.open  method. On line 376  save(writer, datahandle, info)  is advised within a  Base.write(dataset::DataSet, info::T)  method. lint.jl  (1 instance) On line 84  lint(obj, linters)  is advised within a  lint(obj::T)  method. manipulation.jl  (4 instances) On line 53  init(newcollection)  is advised within a  init  method. On line 435  fromspec(DataSet, collection, name, spec)  is advised within a  add  method. On line 572  fromspec(T, dataset, process_spec(spec, drv))  is advised within a  create  method. On line 579  fromspec(T, dataset, process_spec(spec, driver))  is advised within a  create  method. errors.jl  (4 instances) On line 44  parse_ident(err.identifier)  is advised within a  Base.showerror  method. On line 53  parse_ident(err.identifier)  is advised within a  Base.showerror  method. On line 82  string(ident)  is advised within a  Base.showerror  method. On line 90  string(ident)  is advised within a  Base.showerror  method. identification.jl  (4 instances) On line 111  string(ident)  is advised within a  resolve  method. On line 146  refine(matchingdatasets, ident, String[])  is advised within a  refine  method. On line 190  parse_ident(identstr)  is advised within a  resolve  method. On line 195  parse_ident(identstr)  is advised within a  resolve  method. parameters.jl  (2 instances) On line 40  parse_ident(dsid_match.captures[1])  is advised within a  dataset_parameters  method. On line 52  string(param)  is advised within a  dataset_parameters  method. parser.jl  (6 instances) On line 72  parse_ident(spec)  is advised within a  Base.parse  method. On line 118  fromspec(ADT, dataset, spec)  is advised within a  ADT::Type{<:AbstractDataTransformer}  method. On line 163  identity(ADT(dataset, ttype, priority, dataset_parameters(dataset, Val(:extract), parameters)))  is advised within a  fromspec  method. On line 245  identity(collection)  is advised within a  fromspec  method. On line 253  fromspec(DataSet, collection, name, spec)  is advised within a  DataSet  method. On line 283  identity(dataset)  is advised within a  fromspec  method. writer.jl  (3 instances) On line 51  tospec(adt)  is advised within a  Base.convert  method. On line 73  tospec(ds)  is advised within a  Base.convert  method. On line 87  tospec(dc)  is advised within a  Base.convert  method."},{"id":96,"pagetitle":"Data.toml","title":"Data.toml","ref":"/DataToolkitDocs/base/stable/datatoml/#Data.toml","content":" Data.toml A collection of data sets may be encapsulated in a  Data.toml  file, the structure of which is described here."},{"id":97,"pagetitle":"Data.toml","title":"Overall structure","ref":"/DataToolkitDocs/base/stable/datatoml/#Overall-structure","content":" Overall structure data_config_version=0\n\nname=\"data collection name\"\nuuid=\"a UUIDv4\"\nplugins=[\"plugin1\", \"plugin2\", ...]\n\n[config]\n# [Properties of the data collection itself]\n\n[[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties...\n\n[[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\ntype=[\"a QualifiedType\", ...]\npriority=1 # (optional)\n# other properties...\n\n[[mydataset]]\n# There may be multiple data sets by the same name,\n# but they must be uniquely identifyable by their properties\n\n[[exampledata]]\n# Another data set"},{"id":98,"pagetitle":"Data.toml","title":"Attributes of the data collection","ref":"/DataToolkitDocs/base/stable/datatoml/#Attributes-of-the-data-collection","content":" Attributes of the data collection There are four top-level non- table  properties currently recognised. data_config_version  :: The (integer) version of the format. Currently  0  as this project is still in the alpha phase of development, moving towards beta. name  :: an identifying string. Cannot contain  : , and characters outside of  [A-Za-z0-9_]  are recommended against. uuid :: a UUIDv4 used to uniquely refer to the data collection, should it be renamed etc. plugins :: a list of plugins which should be used when working with this data collection In addition to these four, a special table of the name  config  is recognised. This holds custom attributes of the data collection, e.g. [config]\nmykey=\"value\"\n\n[config.defaults]\ndescription=\"Ooops, somebody forgot to describe this.\"\n\n[config.defaults.storage.filesystem]\npriority=2 Note that as a consequence of this special table, no data set may be named \"config\". DataToolkitBase reserves exactly one config attribute:  locked . This is used to indicate that the  Data.toml  file should not be modified, and to override it the attribute must be changed within the  Data.toml  file. By setting  config.locked = true , you protecct yourself from accidental modifications to the data file. This functionality is provided here rather than in DataToolkitsCommon etc. because it supported via the implementation of  Base.iswritable(::DataCollection) , and so downstream packages would only be able to support this by overriding this method."},{"id":99,"pagetitle":"Data.toml","title":"Structure of a data set","ref":"/DataToolkitDocs/base/stable/datatoml/#Structure-of-a-data-set","content":" Structure of a data set [[mydataset]]\nuuid=\"a UUIDv4\"\n# other properties...\n\n[[mydataset.TRANSFORMER]]\ndriver=\"transformer driver\"\ntype=[\"a QualifiedType\", ...] # probably optional\ntype=\"a QualifiedType\" # single-value alternative form\npriority=1 # (optional)\n# other properties... A data set is a top-level instance of an  array of tables , with any name other than  config . Data set names need not be unique, but should be able to be uniquely identified by the combination of their name and parameters. Apart from data transformers, there is one recognised data property:  uuid , a UUIDv4 string. Any number of additional properties may be given (so long as they do not conflict with the transformer names), they may have special behaviour based on plugins or extensions loaded, but will not be treated specially by DataToolkitBase. A data set can have any number of data transformers, but at least two are needed for a functional data set. Data transformers are instances of an array of tables (like data sets), but directly under the data set table."},{"id":100,"pagetitle":"Data.toml","title":"Structure of a data transformer","ref":"/DataToolkitDocs/base/stable/datatoml/#Structure-of-a-data-transformer","content":" Structure of a data transformer There are three data transformers types, with the following names: storage loader writer All transformers recognise three properties: driver , the transformer driver name, as a string type , a single  QualifiedType  string, or an array of them priority , an integer which sets the order in which multiple transformers should be considered The  driver  property is mandatory.  type  and  priority  can be omitted, in which case they will adopt the default values. The default  type  value is either determined dynamically from the available methods, or set for that particular transformer."},{"id":103,"pagetitle":"Errors","title":"Errors","ref":"/DataToolkitDocs/base/stable/errors/#Errors","content":" Errors This package tries to minimise the use of generic errors, and maximise the helpfulness of error messages. To that end, a number of new error types are defined."},{"id":104,"pagetitle":"Errors","title":"Identifier exceptions","ref":"/DataToolkitDocs/base/stable/errors/#Identifier-exceptions","content":" Identifier exceptions"},{"id":105,"pagetitle":"Errors","title":"DataToolkitBase.UnresolveableIdentifier","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.UnresolveableIdentifier","content":" DataToolkitBase.UnresolveableIdentifier  —  Type UnresolveableIdentifier{T}(identifier::Union{String, UUID}, [collection::DataCollection]) No  T  (optionally from  collection ) could be found that matches  identifier . Example occurrences julia> d\"iirs\"\nERROR: UnresolveableIdentifier: \"iirs\" does not match any available data sets\n  Did you perhaps mean to refer to one of these data sets?\n    ■:iris (75% match)\nStacktrace: [...]\n\njulia> d\"iris::Int\"\nERROR: UnresolveableIdentifier: \"iris::Int\" does not match any available data sets\n  Without the type restriction, however, the following data sets match:\n    dataset:iris, which is available as a DataFrame, Matrix, CSV.File\nStacktrace: [...] source"},{"id":106,"pagetitle":"Errors","title":"DataToolkitBase.AmbiguousIdentifier","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.AmbiguousIdentifier","content":" DataToolkitBase.AmbiguousIdentifier  —  Type AmbiguousIdentifier(identifier::Union{String, UUID}, matches::Vector, [collection]) Searching for  identifier  (optionally within  collection ), found multiple matches (provided as  matches ). Example occurrence julia> d\"multimatch\"\nERROR: AmbiguousIdentifier: \"multimatch\" matches multiple data sets\n    ■:multimatch [45685f5f-e6ff-4418-aaf6-084b847236a8]\n    ■:multimatch [92be4bda-55e9-4317-aff4-8d52ee6a5f2c]\nStacktrace: [...] source"},{"id":107,"pagetitle":"Errors","title":"Package exceptions","ref":"/DataToolkitDocs/base/stable/errors/#Package-exceptions","content":" Package exceptions"},{"id":108,"pagetitle":"Errors","title":"DataToolkitBase.UnregisteredPackage","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.UnregisteredPackage","content":" DataToolkitBase.UnregisteredPackage  —  Type UnregisteredPackage(pkg::Symbol, mod::Module) The package  pkg  was asked for within  mod , but has not been registered by  mod , and so cannot be loaded. Example occurrence julia> @import Foo\nERROR: UnregisteredPackage: Foo has not been registered by Main, see @addpkg for more information\nStacktrace: [...] source"},{"id":109,"pagetitle":"Errors","title":"DataToolkitBase.MissingPackage","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.MissingPackage","content":" DataToolkitBase.MissingPackage  —  Type MissingPackage(pkg::Base.PkgId) The package  pkg  was asked for, but does not seem to be available in the current environment. Example occurrence julia> @addpkg Bar \"00000000-0000-0000-0000-000000000000\"\nBar [00000000-0000-0000-0000-000000000000]\n\njulia> @import Bar\n[ Info: Lazy-loading Bar [00000000-0000-0000-0000-000000000001]\nERROR: MissingPackage: Bar [00000000-0000-0000-0000-000000000001] has been required, but does not seem to be installed.\nStacktrace: [...] source"},{"id":110,"pagetitle":"Errors","title":"Data Operation exceptions","ref":"/DataToolkitDocs/base/stable/errors/#Data-Operation-exceptions","content":" Data Operation exceptions"},{"id":111,"pagetitle":"Errors","title":"DataToolkitBase.CollectionVersionMismatch","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.CollectionVersionMismatch","content":" DataToolkitBase.CollectionVersionMismatch  —  Type CollectionVersionMismatch(version::Int) The  version  of the collection currently being acted on is not supported by the current version of DataToolkitBase. Example occurrence julia> fromspec(DataCollection, SmallDict{String, Any}(\"data_config_version\" => -1))\nERROR: CollectionVersionMismatch: -1 (specified) ≠ 0 (current)\n  The data collection specification uses the v-1 data collection format, however\n  the installed DataToolkitBase version expects the v0 version of the format.\n  In the future, conversion facilities may be implemented, for now though you\n  will need to manually upgrade the file to the v0 format.\nStacktrace: [...] source"},{"id":112,"pagetitle":"Errors","title":"DataToolkitBase.EmptyStackError","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.EmptyStackError","content":" DataToolkitBase.EmptyStackError  —  Type EmptyStackError() An attempt was made to perform an operation on a collection within the data stack, but the data stack is empty. Example occurrence julia> getlayer(nothing) # with an empty STACK\nERROR: EmptyStackError: The data collection stack is empty\nStacktrace: [...] source"},{"id":113,"pagetitle":"Errors","title":"DataToolkitBase.ReadonlyCollection","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.ReadonlyCollection","content":" DataToolkitBase.ReadonlyCollection  —  Type ReadonlyCollection(collection::DataCollection) Modification of  collection  is not viable, as it is read-only. Example Occurrence julia> lockedcollection = DataCollection(SmallDict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128)), \"config\" => SmallDict{String, Any}(\"locked\" => true)))\njulia> write(lockedcollection)\nERROR: ReadonlyCollection: The data collection unnamed#298 is locked\nStacktrace: [...] source"},{"id":114,"pagetitle":"Errors","title":"DataToolkitBase.TransformerError","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.TransformerError","content":" DataToolkitBase.TransformerError  —  Type TransformerError(msg::String) A catch-all for issues involving data transformers, with details given in  msg . Example occurrence julia> emptydata = DataSet(DataCollection(), \"empty\", SmallDict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata)\nERROR: TransformerError: Data set \"empty\" could not be loaded in any form.\nStacktrace: [...] source"},{"id":115,"pagetitle":"Errors","title":"DataToolkitBase.UnsatisfyableTransformer","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.UnsatisfyableTransformer","content":" DataToolkitBase.UnsatisfyableTransformer  —  Type UnsatisfyableTransformer{T}(dataset::DataSet, types::Vector{QualifiedType}) A transformer (of type  T ) that could provide any of  types  was asked for, but there is no transformer that satisfies this restriction. Example occurrence julia> emptydata = DataSet(DataCollection(), \"empty\", SmallDict{String, Any}(\"uuid\" => Base.UUID(rand(UInt128))))\nDataSet empty\n\njulia> read(emptydata, String)\nERROR: UnsatisfyableTransformer: There are no loaders for \"empty\" that can provide a String. The defined loaders are as follows:\nStacktrace: [...] source"},{"id":116,"pagetitle":"Errors","title":"DataToolkitBase.OrphanDataSet","ref":"/DataToolkitDocs/base/stable/errors/#DataToolkitBase.OrphanDataSet","content":" DataToolkitBase.OrphanDataSet  —  Type OrphanDataSet(dataset::DataSet) The data set ( dataset ) is no longer a child of its parent collection. This error should not occur, and is intended as a sanity check should something go quite wrong. source"},{"id":119,"pagetitle":"Internals","title":"Private API","ref":"/DataToolkitDocs/base/stable/libinternal/#Private-API","content":" Private API"},{"id":120,"pagetitle":"Internals","title":"Abstract Data Transformer","ref":"/DataToolkitDocs/base/stable/libinternal/#Abstract-Data-Transformer","content":" Abstract Data Transformer"},{"id":121,"pagetitle":"Internals","title":"DataToolkitBase.AbstractDataTransformer","ref":"/DataToolkitDocs/base/stable/libinternal/#DataToolkitBase.AbstractDataTransformer","content":" DataToolkitBase.AbstractDataTransformer  —  Type The supertype for methods producing or consuming data.                  ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information\n                 ▲               ╷\n                 ╰────writer─────╯ There are three subtypes: DataStorage DataLoader DataWrite Each subtype takes a  Symbol  type parameter designating the driver which should be used to perform the data operation. In addition, each subtype has the following fields: dataset::DataSet , the data set the method operates on type::Vector{<:QualifiedType} , the Julia types the method supports priority::Int , the priority with which this method should be used, compared to alternatives. Lower values have higher priority. parameters::SmallDict{String, Any} , any parameters applied to the method. source"},{"id":122,"pagetitle":"Internals","title":"Advice Amalgamation","ref":"/DataToolkitDocs/base/stable/libinternal/#Advice-Amalgamation","content":" Advice Amalgamation"},{"id":123,"pagetitle":"Internals","title":"DataToolkitBase.AdviceAmalgamation","ref":"/DataToolkitDocs/base/stable/libinternal/#DataToolkitBase.AdviceAmalgamation","content":" DataToolkitBase.AdviceAmalgamation  —  Type A collection of  Advices  sourced from available Plugins. Like individual  Advices , a  AdviceAmalgamation  can be called as a function. However, it also supports the following convenience syntax: (::AdviceAmalgamation)(f::Function, args...; kargs...) # -> result Constructors AdviceAmalgamation(adviseall::Function, advisors::Vector{Advice},\n                   plugins_wanted::Vector{String}, plugins_used::Vector{String})\nAdviceAmalgamation(plugins::Vector{String})\nAdviceAmalgamation(collection::DataCollection) source"},{"id":124,"pagetitle":"Internals","title":"Qualified Types","ref":"/DataToolkitDocs/base/stable/libinternal/#Qualified-Types","content":" Qualified Types"},{"id":125,"pagetitle":"Internals","title":"DataToolkitBase.QualifiedType","ref":"/DataToolkitDocs/base/stable/libinternal/#DataToolkitBase.QualifiedType","content":" DataToolkitBase.QualifiedType  —  Type A representation of a Julia type that does not need the type to be defined in the Julia session, and can be stored as a string. This is done by storing the type name and the module it belongs to as Symbols. Warning While  QualifiedType  is currently quite capable, it is not currently able to express the full gamut of Julia types. In future this will be improved, but it will likely always be restricted to a certain subset. Subtyping While the subtype operator cannot work on QualifiedTypes ( <:  is a built-in), when the Julia types are defined the subset operator  ⊆  can be used instead. This works by simply  convert ing the QualifiedTypes to the corresponding Type and then applying the subtype operator. julia> QualifiedTypes(:Base, :Vector) ⊆ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> Matrix ⊆ QualifiedTypes(:Core, :Array)\ntrue\n\njulia> QualifiedTypes(:Base, :Vector) ⊆ AbstractVector\ntrue\n\njulia> QualifiedTypes(:Base, :Foobar) ⊆ AbstractVector\nfalse Constructors QualifiedType(parentmodule::Symbol, typename::Symbol)\nQualifiedType(t::Type) Parsing A QualifiedType can be expressed as a string as  \"$parentmodule.$typename\" . This can be easily  parse d as a QualifiedType, e.g.  parse(QualifiedType, \"Core.IO\") . source"},{"id":126,"pagetitle":"Internals","title":"Global variables","ref":"/DataToolkitDocs/base/stable/libinternal/#Global-variables","content":" Global variables"},{"id":127,"pagetitle":"Internals","title":"DataToolkitBase.STACK","ref":"/DataToolkitDocs/base/stable/libinternal/#DataToolkitBase.STACK","content":" DataToolkitBase.STACK  —  Constant The set of data collections currently available. source"},{"id":128,"pagetitle":"Internals","title":"DataToolkitBase.PLUGINS","ref":"/DataToolkitDocs/base/stable/libinternal/#DataToolkitBase.PLUGINS","content":" DataToolkitBase.PLUGINS  —  Constant The set of plugins currently available. source"},{"id":129,"pagetitle":"Internals","title":"DataToolkitBase.EXTRA_PACKAGES","ref":"/DataToolkitDocs/base/stable/libinternal/#DataToolkitBase.EXTRA_PACKAGES","content":" DataToolkitBase.EXTRA_PACKAGES  —  Constant The set of packages loaded by each module via  @addpkg , for import with  @import . More specifically, when a module M invokes  @addpkg pkg id  then  EXTRA_PACKAGES[M][pkg] = id  is set, and then this information is used with  @import  to obtain the package from the root module. source"},{"id":130,"pagetitle":"Internals","title":"DataToolkitBase.DATA_CONFIG_RESERVED_ATTRIBUTES","ref":"/DataToolkitDocs/base/stable/libinternal/#DataToolkitBase.DATA_CONFIG_RESERVED_ATTRIBUTES","content":" DataToolkitBase.DATA_CONFIG_RESERVED_ATTRIBUTES  —  Constant The data specification TOML format constructs a DataCollection, which itself contains DataSets, comprised of metadata and AbstractDataTransformers. DataCollection\n├─ DataSet\n│  ├─ AbstractDataTransformer\n│  └─ AbstractDataTransformer\n├─ DataSet\n⋮ Within each scope, there are certain reserved attributes. They are listed in this Dict under the following keys: :collection  for  DataCollection :dataset  for  DataSet :transformer  for  AbstractDataTransformer source"},{"id":133,"pagetitle":"Transformer backends","title":"Creating a new data transformer","ref":"/DataToolkitDocs/base/stable/newtransformer/#Creating-a-new-data-transformer","content":" Creating a new data transformer As mentioned before, there are three types of data transformer: storage loader writer The three corresponding Julia types are: DataStorage DataLoader DataWriter All three types accept a  driver  (symbol) type parameter. For example, a storage transformer using a \"filesystem\" driver would be of the type  DataStorage{:filesystem} . Adding support for a new driver is a simple as adding method implementations for the three key data transformer methods:"},{"id":134,"pagetitle":"Transformer backends","title":"DataToolkitBase.load","ref":"/DataToolkitDocs/base/stable/newtransformer/#DataToolkitBase.load","content":" DataToolkitBase.load  —  Function load(loader::DataLoader{driver}, source::Any, as::Type) Using a certain  loader , obtain information in the form of  as  from the data given by  source . This fulfils this component of the overall data flow:   ╭────loader─────╮\n  ╵               ▼\nData          Information When the loader produces  nothing  this is taken to indicate that it was unable to load the data for some reason, and that another loader should be tried if possible. This can be considered a soft failure. Any other value is considered valid information. source"},{"id":135,"pagetitle":"Transformer backends","title":"DataToolkitBase.storage","ref":"/DataToolkitDocs/base/stable/newtransformer/#DataToolkitBase.storage","content":" DataToolkitBase.storage  —  Function storage(storer::DataStorage, as::Type; write::Bool=false) Fetch a  storer  in form  as , appropiate for reading from or writing to (depending on  write ). By default, this just calls  getstorage  or  putstorage  (when  write=true ). This executes this component of the overall data flow: Storage ◀────▶ Data source"},{"id":136,"pagetitle":"Transformer backends","title":"DataToolkitBase.save","ref":"/DataToolkitDocs/base/stable/newtransformer/#DataToolkitBase.save","content":" DataToolkitBase.save  —  Function save(writer::Datasaveer{driver}, destination::Any, information::Any) Using a certain  writer , save the  information  to the  destination . This fulfils this component of the overall data flow: Data          Information\n  ▲               ╷\n  ╰────writer─────╯ source"},{"id":139,"pagetitle":"Packages","title":"Using Packages","ref":"/DataToolkitDocs/base/stable/packages/#Using-Packages","content":" Using Packages It is entirely likely that in the course of writing a package providing a custom data transformer, one would come across packages that  may  be needed. Every possibly desired package could be shoved into the list of dependences, but this is a somewhat crude approach. A more granular approach is enabled with two macros,  @addpkg  and  @import ."},{"id":140,"pagetitle":"Packages","title":"Letting DataToolkitBase know about extra packages","ref":"/DataToolkitDocs/base/stable/packages/#Letting-DataToolkitBase-know-about-extra-packages","content":" Letting DataToolkitBase know about extra packages"},{"id":141,"pagetitle":"Packages","title":"DataToolkitBase.@addpkg","ref":"/DataToolkitDocs/base/stable/packages/#DataToolkitBase.@addpkg","content":" DataToolkitBase.@addpkg  —  Macro @addpkg name::Symbol uuid::String Register the package identified by  name  with UUID  uuid . This package may now be used with  @import $name . All @addpkg statements should lie within a module's  __init__  function. Example @addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\" source"},{"id":142,"pagetitle":"Packages","title":"Using extra packages","ref":"/DataToolkitDocs/base/stable/packages/#Using-extra-packages","content":" Using extra packages"},{"id":143,"pagetitle":"Packages","title":"DataToolkitBase.@import","ref":"/DataToolkitDocs/base/stable/packages/#DataToolkitBase.@import","content":" DataToolkitBase.@import  —  Macro @import pkg1, pkg2...\n@import pkg1 as name1, pkg2 as name2...\n@import pkg: foo, bar...\n@import pkg: foo as bar, bar as baz... Fetch modules previously registered with  @addpkg , and import them into the current namespace. This macro tries to largely mirror the syntax of  using . If a required package had to be loaded for the  @import  statement, a  PkgRequiredRerunNeeded  singleton will be returned. Example @import pkg\npkg.dothing(...)\n# Alternative form\n@import pkg: dothing\ndothing(...) source"},{"id":144,"pagetitle":"Packages","title":"Example","ref":"/DataToolkitDocs/base/stable/packages/#Example","content":" Example module DataToolkitExample\n\nusing DataToolkitBase\nusing DataFrame\n\nfunction __init__()\n    @addpkg CSV \"336ed68f-0bac-5ca0-87d4-7b16caf5d00b\"\n    @addpkg DelimitedFiles \"8bb1440f-4735-579b-a4ab-409b98df4dab\"\nend\n\nfunction load(::DataLoader{:csv}, from::IOStream, ::Type{DataFrame})\n    @import CSV\n    result = CSV.read(from, DataFrame)\n    close(from)\n    result\nend\n\nfunction load(::DataLoader{:delimcsv}, from::IOStream, ::Type{DataFrame})\n    @import DelimitedFiles\n    result = DelimitedFiles.readdlm(from, ',', DataFrame)\n    close(from)\n    result\nend\n\nend"},{"id":147,"pagetitle":"REPL","title":"The Data REPL","ref":"/DataToolkitDocs/base/stable/repl/#The-Data-REPL","content":" The Data REPL"},{"id":148,"pagetitle":"REPL","title":"General design","ref":"/DataToolkitDocs/base/stable/repl/#General-design","content":" General design An extensible \"Data REPL\" is provided to make directly interacting with the  Data.toml  a bit more convenient. It can be entered by pressing  }  on an empty  julia>  REPL line. julia> # type }\ndata>\ndata> help\n Command  Action\n ───────────────────────────────────────────────────────────\n help     Display help information on the available commands The foremost data collection is also listed in the prompt in much the same manner as  (environment) pkg> , i.e. (⋅) data>        # No currently loaded data collections\n(example) data>  # The top data collection is \"example\" Commands (and sub-commands) can be triggered by typing them out in full (i.e.  cmd args... ) but also abbreviated up to the unique stem. For instance if  cmd  is the only command starting with  c , then it can be called with any of data> cmd args... data> cm args... data> c args... However, should a command  conflict  also exist, then  c  is no longer a unique stem and so  c args...  will produce an error message like so: data> c args...\n ! Multiple matching Data REPL commands: cmd, conflict"},{"id":149,"pagetitle":"REPL","title":"The help command","ref":"/DataToolkitDocs/base/stable/repl/#The-help-command","content":" The help command help  is implemented specially in the Data REPL. It can be invoked normally (i.e.  help cmd ) but also with  ?  prefix (i.e.  ?cmd ). Furthermore, all commands with sub-commands with automatically have a  help  sub-command added. Overall,  help  supports the following usage patterns. data> help             # List available commands.\ndata> help cmd         # Obtain the help for cmd, or\ndata> help cmd         # list available sub-command (if applicable).\ndata> ?cmd             # Obtain the help for cmd.\ndata> help cmd subcmd  # Obtain the help for subcmd.\ndata> ?cmd subcmd      # Obtain the help for subcmd.\ndata> cmd help subcmd  # Obtain the help for subcmd.\ndata> cmd ?subcmd      # Obtain the help for subcmd."},{"id":150,"pagetitle":"REPL","title":"Extending the Data REPL","ref":"/DataToolkitDocs/base/stable/repl/#Extending-the-Data-REPL","content":" Extending the Data REPL"},{"id":151,"pagetitle":"REPL","title":"Registering commands","ref":"/DataToolkitDocs/base/stable/repl/#Registering-commands","content":" Registering commands To register a command, one simply needs to push a  ReplCmd  onto  REPL_CMDS ."},{"id":152,"pagetitle":"REPL","title":"DataToolkitBase.REPL_CMDS","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.REPL_CMDS","content":" DataToolkitBase.REPL_CMDS  —  Constant The set of commands available directly in the Data REPL. source"},{"id":153,"pagetitle":"REPL","title":"DataToolkitBase.ReplCmd","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.ReplCmd","content":" DataToolkitBase.ReplCmd  —  Type A command that can be used in the Data REPL (accessible through '}'). A  ReplCmd  must have a: name , a symbol designating the command keyword. trigger , a string used as the command trigger (defaults to  String(name) ). description , a short overview of the functionality as a  string  or  display able object. execute , either a list of sub-ReplCmds, or a function which will perform the command's action. The function must take a single argument, the rest of the command as an  AbstractString  (for example, 'cmd arg1 arg2' will call the execute function with \"arg1 arg2\"). Constructors ReplCmd{name::Symbol}(trigger::String, description::Any, execute::Function)\nReplCmd{name::Symbol}(description::Any, execute::Function)\nReplCmd(name::Union{Symbol, String}, trigger::String, description::Any, execute::Function)\nReplCmd(name::Union{Symbol, String}, description::Any, execute::Function) Examples ReplCmd(:echo, \"print the argument\", identity)\nReplCmd(:addone, \"return the input plus one\", v -> 1 + parse(Int, v))\nReplCmd(:math, \"A collection of basic integer arithmetic\",\n    [ReplCmd(:add, \"a + b + ...\", nums -> sum(parse.(Int, split(nums))))],\n     ReplCmd(:mul, \"a * b * ...\", nums -> prod(parse.(Int, split(nums))))) Methods help(::ReplCmd) # -> print detailed help\nallcompletions(::ReplCmd) # -> list all candidates\ncompletions(::ReplCmd, sofar::AbstractString) # -> list relevant candidates source"},{"id":154,"pagetitle":"REPL","title":"Completion","ref":"/DataToolkitDocs/base/stable/repl/#Completion","content":" Completion As hinted by the  ReplCmd  docstring, completions can be implemented by implementing  completions(::ReplCmd{:CMD_ID}, sofar::AbstractString)  or  allcompletions ."},{"id":155,"pagetitle":"REPL","title":"DataToolkitBase.completions","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.completions","content":" DataToolkitBase.completions  —  Function completions(r::ReplCmd, sofar::AbstractString) Obtain a list of  String  completion candidates based on  sofar . All candidates should begin with  sofar . Should this function not be implemented for the specific ReplCmd  r ,  allcompletions(r)  will be called and filter to candidates that begin with  sofar . If  r  has subcommands, then the subcommand prefix will be removed and  completions  re-called on the relevant subcommand. source"},{"id":156,"pagetitle":"REPL","title":"DataToolkitBase.allcompletions","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.allcompletions","content":" DataToolkitBase.allcompletions  —  Function allcompletions(r::ReplCmd) Obtain all possible  String  completion candidates for  r . This defaults to the empty vector  String[] . allcompletions  is only called when  completions(r, sofar::AbstractString)  is not implemented. source"},{"id":157,"pagetitle":"REPL","title":"Helper functions","ref":"/DataToolkitDocs/base/stable/repl/#Helper-functions","content":" Helper functions To create a pleasant user interface, a number of utility functions are provided."},{"id":158,"pagetitle":"REPL","title":"DataToolkitBase.prompt","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.prompt","content":" DataToolkitBase.prompt  —  Function prompt(question::AbstractString, default::AbstractString=\"\",\n       allowempty::Bool=false, cleardefault::Bool=true,\n       multiline::Bool=false) Interactively ask  question  and return the response string, optionally with a  default  value. If  multiline  is true,  RET  must be pressed twice consecutively to submit a value. Unless  allowempty  is set an empty response is not accepted. If  cleardefault  is set, then an initial backspace will clear the default value. The prompt supports the following line-edit-y keys: left arrow right arrow home end delete forwards delete backwards Example julia> prompt(\"What colour is the sky? \")\nWhat colour is the sky? Blue\n\"Blue\" source"},{"id":159,"pagetitle":"REPL","title":"DataToolkitBase.prompt_char","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.prompt_char","content":" DataToolkitBase.prompt_char  —  Function prompt_char(question::AbstractString, options::Vector{Char},\n            default::Union{Char, Nothing}=nothing) Interactively ask  question , only accepting  options  keys as answers. All keys are converted to lower case on input. If  default  is not nothing and 'RET' is hit, then  default  will be returned. Should '^C' be pressed, an InterruptException will be thrown. source"},{"id":160,"pagetitle":"REPL","title":"DataToolkitBase.confirm_yn","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.confirm_yn","content":" DataToolkitBase.confirm_yn  —  Function confirm_yn(question::AbstractString, default::Bool=false) Interactively ask  question  and accept y/Y/n/N as the response. If any other key is pressed, then  default  will be taken as the response. A \" [y/n]: \" string will be appended to the question, with y/n capitalised to indicate the default value. Example julia> confirm_yn(\"Do you like chocolate?\", true)\nDo you like chocolate? [Y/n]: y\ntrue source"},{"id":161,"pagetitle":"REPL","title":"DataToolkitBase.peelword","ref":"/DataToolkitDocs/base/stable/repl/#DataToolkitBase.peelword","content":" DataToolkitBase.peelword  —  Function peelword(input::AbstractString) Read the next 'word' from  input . If  input  starts with a quote, this is the unescaped text between the opening and closing quote. Other wise this is simply the next word. Returns a tuple of the form  (word, rest) . Example julia> peelword(\"one two\")\n(\"one\", \"two\")\n\njulia> peelword(\"\"one two\" three\")\n(\"one two\", \"three\") source"},{"id":162,"pagetitle":"REPL","title":"Simple example","ref":"/DataToolkitDocs/base/stable/repl/#Simple-example","content":" Simple example In the below example we will extend the Data REPL by adding a command  cowsay  which simply call the (assumed to be installed) system  cowsay  executable. function cowsay_repl(input::AbstractString)\n    if isempty(input)\n        confirm_yn(\"Are you ready to hear your fortune?\", true) &&\n            cowsay_repl(read(`fortune`, String))\n    else\n        println(read(`cowsay $input`, String))\n    end\nend\n\npush!(REPL_CMDS, ReplCmd(:cowsay3,\n                         \"Hear what the cow has to say\n\\n Call with no argument to obtain a fortune.\",\n                         cowsay_repl))\n\nDataToolkitBase.allcompletions(::ReplCmd{:cowsay}) =\n    [\"Improve your data management with DataToolkits & co.\"] If you enter the Data REPL, you will be able to note that: cowsay  is listed in  data> help running  cowsay  with no arguments results in a Y/n prompt to show a fortune pressing  TAB  after  cowsay  fills in the sole completion,  Improve your data   management with DataToolkits & co. . (⋅) data> help\n Command  Action\n ───────────────────────────────────────────────────────────\n cowsay   Hear what the cow has to say\n help     Display help information on the available commands\n\n(⋅) data> ?cowsay3\n Hear what the cow has to say\n\n Call with no argument to obtain a fortune.\n\n(⋅) data> cowsay\nAre you ready to hear your fortune? [Y/n]: y\n _________________________________________\n/ (1) A sheet of paper is an ink-lined    \\\n| plane. (2) An inclined plane is a slope |\n| up. (3) A slow pup is a lazy dog.       |\n|                                         |\n| QED: A sheet of paper is a lazy dog.    |\n|                                         |\n| -- Willard Espy, \"An Almanac of Words   |\n\\ at Play\"                                /\n -----------------------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n"},{"id":165,"pagetitle":"Usage","title":"Usage","ref":"/DataToolkitDocs/base/stable/usage/#Usage","content":" Usage"},{"id":166,"pagetitle":"Usage","title":"Identifying a dataset","ref":"/DataToolkitDocs/base/stable/usage/#Identifying-a-dataset","content":" Identifying a dataset"},{"id":167,"pagetitle":"Usage","title":"Reading datasets","ref":"/DataToolkitDocs/base/stable/usage/#Reading-datasets","content":" Reading datasets"},{"id":168,"pagetitle":"Usage","title":"Base.read","ref":"/DataToolkitDocs/base/stable/usage/#Base.read","content":" Base.read  —  Function read(filename::AbstractString, DataCollection; writer::Union{Function, Nothing}) Read the entire contents of a file as a  DataCollection . The default value of writer is  self -> write(filename, self) . source read(io::IO, DataCollection; path::Union{String, Nothing}=nothing, mod::Module=Base.Main) Read the entirety of  io , as a  DataCollection . source read(dataset::DataSet, as::Type)\nread(dataset::DataSet) # as default type Obtain information from  dataset  in the form of  as , with the appropriate loader and storage provider automatically determined. This executes this component of the overall data flow:                  ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information The loader and storage provider are selected by identifying the highest priority loader that can be satisfied by a storage provider. What this looks like in practice is illustrated in the diagram below.       read(dataset, Matrix) ⟶ ::Matrix ◀╮\n         ╭───╯        ╰────────────▷┬───╯\n╔═════╸dataset╺══════════════════╗  │\n║ STORAGE      LOADERS           ║  │\n║ (⟶ File)─┬─╮ (File ⟶ String)   ║  │\n║ (⟶ IO)   ┊ ╰─(File ⟶ Matrix)─┬─╫──╯\n║ (⟶ File)┄╯   (IO ⟶ String)   ┊ ║\n║              (IO ⟶ Matrix)╌╌╌╯ ║\n╚════════════════════════════════╝\n\n  ─ the load path used\n  ┄ an option not taken\n\nTODO explain further source"},{"id":169,"pagetitle":"Usage","title":"Writing datasets","ref":"/DataToolkitDocs/base/stable/usage/#Writing-datasets","content":" Writing datasets"},{"id":170,"pagetitle":"Usage","title":"Base.write","ref":"/DataToolkitDocs/base/stable/usage/#Base.write","content":" Base.write  —  Function write(dataset::DataSet, info::Any) TODO write docstring source"},{"id":171,"pagetitle":"Usage","title":"Accessing the raw data","ref":"/DataToolkitDocs/base/stable/usage/#Accessing-the-raw-data","content":" Accessing the raw data"},{"id":172,"pagetitle":"Usage","title":"Base.open","ref":"/DataToolkitDocs/base/stable/usage/#Base.open","content":" Base.open  —  Function open(dataset::DataSet, as::Type; write::Bool=false) Obtain the data of  dataset  in the form of  as , with the appropriate storage provider automatically selected. A  write  flag is also provided, to help the driver pick a more appropriate form of  as . This executes this component of the overall data flow:                  ╭────loader─────╮\n                 ╵               ▼\nStorage ◀────▶ Data          Information source"},{"id":175,"pagetitle":"Introduction","title":"Introduction","ref":"/DataToolkitDocs/common/stable/#Introduction","content":" Introduction DataToolkitCommon takes the skeleton provided by DataToolkitBase and builds the basic functionality needed to make DataToolkit actually useful. To gain an understanding of how these two components fit together, it is recommended the DataToolkitBase docs be looked at first to understand the system, and then these docs for useful transformers and plugins to actually work with your data."},{"id":178,"pagetitle":"Cache","title":"Cache","ref":"/DataToolkitDocs/common/stable/plugins/cache/#Cache","content":" Cache Cache the results of data loaders using the  Serialisation  standard library. Cache keys are determined by the loader \"recipe\" and the type requested. It is important to note that not all data types can be cached effectively, such as an  IOStream . Recipe hashing The driver, parameters, type(s), of a loader and the storage drivers of a dataset are all combined into the \"recipe hash\" of a loader. ╭─────────╮             ╭──────╮\n│ Storage │             │ Type │\n╰───┬─────╯             ╰───┬──╯\n    │    ╭╌╌╌╌╌╌╌╌╌╮    ╭───┴────╮ ╭────────╮\n    ├╌╌╌╌┤ DataSet ├╌╌╌╌┤ Loader ├─┤ Driver │\n    │    ╰╌╌╌╌╌╌╌╌╌╯    ╰───┬────╯ ╰────────╯\n╭───┴─────╮             ╭───┴───────╮\n│ Storage ├─╼           │ Parmeters ├─╼\n╰─────┬───╯             ╰───────┬───╯\n      ╽                         ╽ Since the parameters of the loader (and each storage backend) can reference other data sets (indicated with  ╼  and  ╽ ), this hash is computed recursively, forming a Merkle Tree. In this manner the entire \"recipe\" leading to the final result is hashed.                 ╭───╮\n                │ E │\n        ╭───╮   ╰─┬─╯\n        │ B ├──▶──┤\n╭───╮   ╰─┬─╯   ╭─┴─╮\n│ A ├──▶──┤     │ D │\n╰───╯   ╭─┴─╮   ╰───╯\n        │ C ├──▶──┐\n        ╰───╯   ╭─┴─╮\n                │ D │\n                ╰───╯ In this example, the hash for a loader of data set \"A\" relies on the data sets \"B\" and \"C\", and so their hashes are calculated and included. \"D\" is required by both \"B\" and \"C\", and so is included in each. \"E\" is also used in \"D\". Configuration Store path This uses the same  store.path  configuration variable as the  store  plugin (which see). Disabling on a per-loader basis Caching of individual loaders can be disabled by setting the \"cache\" parameter to  false , i.e. [[somedata.loader]]\ncache = false\n... Store management System-wide configuration can be set via the  store config set  REPL command, or directly modifying the  DataToolkitCommon.Store.getinventory().config  struct. A few (system-wide) settings determine garbage collection behaviour: auto_gc  (default 2): How often to automatically run garbage collection (in hours). Set to a non-positive value to disable. max_age  (default 30): The maximum number of days since a collection was last seen before it is removed from consideration. max_size  (default 53687091200): The maximum (total) size of the store. recency_beta  (default 1): When removing items to avoid going over  max_size , how much recency should be valued. Can be set to any value in (-∞, ∞). Larger (positive) values weight recency more, and negative values weight size more. -1 and 1 are equivalent. store_dir  (default store): The directory (either as an absolute path, or relative to the inventory file) that should be used for storage (IO) cache files. cache_dir  (default cache): The directory (either as an absolute path, or relative to the inventory file) that should be used for Julia cache files."},{"id":181,"pagetitle":"Defaults","title":"Defaults","ref":"/DataToolkitDocs/common/stable/plugins/defaults/#Defaults","content":" Defaults Apply default values from the \"defaults\" data collection property. This works with both DataSets and AbstractDataTransformers. Default DataSet property [config.defaults]\ndescription=\"Oh no, nobody bothered to describe this dataset.\" Default AbstractDataTransformer property This is scoped to a particular transformer, and a particular driver. One may also affect all drivers with the special \"all drivers\" key  _ . Specific-driver defaults always override all-driver defaults. [config.defaults.storage._]\npriority=0\n\n[config.defaults.storage.filesystem]\npriority=2"},{"id":184,"pagetitle":"Log","title":"Log","ref":"/DataToolkitDocs/common/stable/plugins/log/#Log","content":" Log Log major data set events. Settings config.log.events = [\"load\", \"save\", \"storage\"] # the default To log all event types unconditionally, simply set  config.log.events  to  true . Loggable events load , when a loader is run save , when a writer is run storage , when storage is accessed, in read or write mode Other transformers or plugins may extend the list of recognised events."},{"id":187,"pagetitle":"Memorise","title":"Memorise","ref":"/DataToolkitDocs/common/stable/plugins/memorise/#Memorise","content":" Memorise Cache the results of data loaders in memory. This requires  (dataset::DataSet, as::Type)  to consistently identify the same loaded information. Enabling caching of a dataset [[mydata]]\nmemorise = true memorise  can be a boolean value, a type that should be memorised, or a list of types to be memorised."},{"id":190,"pagetitle":"Store","title":"Store","ref":"/DataToolkitDocs/common/stable/plugins/store/#Store","content":" Store Cache IO from data storage backends, by saving the contents to the disk. Configuration Store path The directory the the store is maintained in can be set via the  store.path  configuration parameter. config.store.path = \"relative/to/datatoml\" The system default is  ~/.cache/julia/datatoolkit , which can be overriden with the  DATATOOLKIT_STORE  environment variable. Disabling on a per-storage basis Saving of individual storage sources can be disabled by setting the \"save\" parameter to  false , i.e. [[somedata.storage]]\nsave = false Checksums To ensure data integrity, a checksum can be specified, and checked when saving to the store. For example, [[iris.storage]]\nchecksum = \"k12:cfb9a6a302f58e5a9b0c815bb7e8efb4\" If you do not have a checksum, but wish for one to be calculated upon accessing the data, the checksum parameter can be set to the special value  \"auto\" . When the data is first accessed, a checksum will be generated and replace the \"auto\" value. Instead of  \"auto\" , a particular checksum algorithm can be specified, by naming it, e.g.  \"sha256\" . The currently supported algorithms are:  k12  (Kangaroo Twelve),  sha512 ,  sha348 ,  sha256 ,  sha224 ,  sha1 ,  md5 , and  crc32c . To explicitly specify no checksum, set the parameter to  false . Expiry/lifecycle After a storage source is saved, the cache file can be made to expire after a certain period. This is done by setting the \" lifetime \" parameter of the storage, i.e. [[updatingdata.storage]]\nlifetime = \"3 days\" The lifetime parameter accepts a few formats, namely: ISO8061 periods  (with whole numbers only), both forms P[n]Y[n]M[n]DT[n]H[n]M[n]S , e.g. P3Y6M4DT12H30M5S  represents a duration of \"3 years, 6 months, 4 days, 12 hours, 30 minutes, and 5 seconds\" P23DT23H  represents a duration of \"23 days, 23 hours\" P4Y  represents a duration of \"4 years\" PYYYYMMDDThhmmss  /  P[YYYY]-[MM]-[DD]T[hh]:[mm]:[ss] , e.g. P0003-06-04T12:30:05 P00030604T123005 \"Prose style\" period strings , which are a repeated pattern of  [number] [unit] , where  unit  matches  year|y|month|week|wk|w|day|d|hour|h|minute|min|second|sec|  optionally followed by an \"s\", comma, or whitespace. E.g. 3 years 6 months 4 days 12 hours 30 minutes 5 seconds 23 days, 23 hours 4d12h By default, the first lifetime period begins at the Unix epoch. This means a daily lifetime will tick over at  00:00 UTC . The \" lifetime_offset \" parameter can be used to shift this. It can be set to a lifetime string, date/time-stamp, or number of seconds. For example, to have the lifetime expire at  03:00 UTC  instead, the lifetime offset could be set to three hours. [[updatingdata.storage]]\nlifetime = \"1 day\"\nlifetime_offset = \"3h\" We can produce the same effect by specifying a different reference point for the lifetime. [[updatingdata.storage]]\nlifetime = \"1 day\"\nlifetime_offset = 1970-01-01T03:00:00 Store management System-wide configuration can be set via the  store config set  REPL command, or directly modifying the  DataToolkitCommon.Store.getinventory().config  struct. A few (system-wide) settings determine garbage collection behaviour: auto_gc  (default 2): How often to automatically run garbage collection (in hours). Set to a non-positive value to disable. max_age  (default 30): The maximum number of days since a collection was last seen before it is removed from consideration. max_size  (default 53687091200): The maximum (total) size of the store. recency_beta  (default 1): When removing items to avoid going over  max_size , how much recency should be valued. Can be set to any value in (-∞, ∞). Larger (positive) values weight recency more, and negative values weight size more. -1 and 1 are equivalent. store_dir  (default store): The directory (either as an absolute path, or relative to the inventory file) that should be used for storage (IO) cache files. cache_dir  (default cache): The directory (either as an absolute path, or relative to the inventory file) that should be used for Julia cache files."},{"id":193,"pagetitle":"Versions","title":"Versions","ref":"/DataToolkitDocs/common/stable/plugins/versions/#Versions","content":" Versions Give data sets versions, and identify them by version. Giving data sets a version Multiple editions of a data set can be described by using the same name, but setting the  version  parameter to differentiate them. For instance, say that Ronald Fisher released a second version of the \"Iris\" data set, with more flowers. We could specify this as: [[iris]]\nversion = \"1\"\n...\n\n[[iris]]\nversion = \"2\"\n... Matching by version Version matching is done via the  Identifier  parameter  \"version\" . As shorthand, instead of providing the  \"version\"  parameter manually, the version can be tacked onto the end of an identifier with  @ , e.g.  iris@1  or  iris@2 . The version matching re-uses machinery from  Pkg , and so all  Pkg -style version specifications  are supported. In addition to this, one can simply request the \"latest\" version. The following are all valid identifiers, using the  @ -shorthand: iris@1\niris@~1\niris@>=2\niris@latest When multiple data sets match the version specification, the one with the highest matching version is used."},{"id":196,"pagetitle":"REPL","title":"REPL","ref":"/DataToolkitDocs/common/stable/repl/#REPL","content":" REPL All of the relevant help for the REPL can be accessed from inside the REPL. They are repeated here verbatim. help   Command  Action                                         \n  ────────────────────────────────────────────────────────\n  add      Add a data set to the current collection       \n  check    Check the state for potential issues           \n  config   Inspect and modify the current configuration   \n  edit     Edit the specification of a dataset            \n  init     Initialise a new data collection               \n  list     List the datasets in a certain collection      \n  make     Create a new data set from existing information\n  plugin   Inspect and modify the set of plugins used     \n  remove   Remove a data set                              \n  search   Search for a particular data collection        \n  show     Show the dataset refered to by an identifier   \n  stack    Operate on the data collection stack           \n  store    Manipulate the data store                      \n  help     Display help text for commands and transformers\n\n  �[2;3mCommands can also be triggered by unique prefixes or substrings.�[22;23m\n ?add Add a data set to the current collection Usage This will interactively ask for all required information. Optionally, the  name  and  source  can be specified using the following forms: data> add NAME\ndata> add NAME from SOURCE\ndata> add from SOURCE As a shorthand,  f  can be used instead of  from . The transformers drivers used can also be specified by using a  via  argument before  from , with a form like so: data> add via TRANSFORMERS...\ndata> add NAME via TRANSFORMERS... from SOURCE The  type  of transformer can also be specified using flags. Namely storage ( -s ), loader ( -l ), and writer ( -w ). For example: data> add via -s web -l csv Invalid transformer drivers are automatically skipped, so one could use: data> add via -sl web csv which would be equivalent to  add via -s web csv -l web csv , but only  web  will be reccognised as a valid storage backend and  csv  as a valid loader. This works well in most cases, which is why  -sl  are the default flags. Examples data> add iris from https://github.com/mwaskom/seaborn-data/blob/master/iris.csv\ndata> add iris via web csv from https://github.com/mwaskom/seaborn-data/blob/master/iris.csv\ndata> add iris via -s web -l csv from https://github.com/mwaskom/seaborn-data/blob/master/iris.csv\ndata> add \"from\" from.txt # add a data set with the name from ?check Check the state for potential issues By default, this operates on the active collection, however it can also be applied to any other collection or a specific data set. Usage data> check (runs on the active collection)\ndata> check COLLECTION\ndata> check IDENTIFIER ?config   Inspect and modify the current configuration\n\n  Subcommand  Action                                         \n  ───────────────────────────────────────────────────────────\n  get         Get the current configuration                  \n  set         Set a configuration property                   \n  unset       Remove a configuration property                \n  help        Display help text for commands and transformers\n ?config get Get the current configuration The parameter to get the configuration of should be given using TOML-style dot seperation. Examples data> get defaults.memorise\ndata> get my.\"special thing\".extra ?config set Set a configuration property The parameter to set the configuration of should be given using TOML-style dot seperation. Similarly, the new value should be expressed using TOML syntax. ##Examples data> set defaults.memorise true\ndata> set my.\"special thing\".extra {a=1, b=2} ?config unset Remove a configuration property The parameter to be removed should be given using TOML-style dot seperation. Examples data> unset defaults.memorise\ndata> unset my.\"special thing\".extra ?edit Edit the specification of a dataset Open the specified dataset as a TOML file for editing, and reload the dataset from the edited contents. Usage data> edit IDENTIFIER ?init Initialise a new data collection Optionally, a data collection name and path can be specified with the forms: data> init [NAME]\ndata> init [PATH]\ndata> init [NAME] [PATH]\ndata> init [NAME] at [PATH] Plugins can also be specified by adding a  with  argument, data> init [...] with PLUGINS... To omit the default set of plugins, put  with -n  instead, i.e. data> init [...] with -n PLUGINS... Usage data> init\ndata> init /tmp/test\ndata> init test at /tmp/test\ndata> init test at /tmp/test with plugin1 plugin2 ?list List the datasets in a certain collection By default, the datasets of the active collection are shown. Usage data> list (lists dataset of the active collection)\ndata> list COLLECTION ?make Create a new data set from existing information This drops you into a sandbox where you can interactively develop a script to produce a new data set. Usage data> make\ndata> make new_dataset_name ?plugin   Inspect and modify the set of plugins used\n\n  Subcommand  Action                                            \n  ──────────────────────────────────────────────────────────────\n  add         Add plugins to the first data collection          \n  remove      Remove plugins from the first data collection     \n  edit        Edit the plugins used by the first data collection\n  info        Fetch the documentation of a plugin               \n  list        List the plugins used by the first data collection\n  help        Display help text for commands and transformers   \n ?plugin add Add plugins to the first data collection ?plugin remove Remove plugins from the first data collection ?plugin edit Edit the plugins used by the first data collection ?plugin info Fetch the documentation of a plugin ?plugin list List the plugins used by the first data collection\n\nWith '-a'/'--availible' all loaded plugins are listed instead. ?remove Remove a data set Usage data> remove IDENTIFIER ?search Search for a particular data collection Usage data> search TEXT... ?show Show the dataset refered to by an identifier Usage data> show IDENTIFIER ?stack   Operate on the data collection stack\n\n  Subcommand  Action                                          \n  ────────────────────────────────────────────────────────────\n              List the data collections of the data stack     \n  promote     Move an entry up the stack                      \n  demote      Move an entry down the stack                    \n  load        Load a data collection onto the top of the stack\n  remove      Remove an entry from the stack                  \n  help        Display help text for commands and transformers \n ?stack promote Move an entry up the stack An entry can be identified using any of the following: The current position in the stack The name of the data collection The UUID of the data collection The number of positions the entry should be promoted by defaults to 1, but can optionally be specified by putting either an integer or the character  *  after the identifier. When  *  is given, the entry will be promoted to the top of the data stack. Examples with different identifier forms data> promote 2\ndata> promote mydata\ndata> promote 853a9f6a-cd5e-4447-a0a4-b4b2793e0a48 Examples with different promotion degrees data> promote mydata\ndata> promote mydata 3\ndata> promote mydata * ?stack demote Move an entry down the stack An entry can be identified using any of the following: The current position in the stack The name of the data collection The UUID of the data collection The number of positions the entry should be demoted by defaults to 1, but can optionally be specified by putting either an integer or the character  *  after the identifier. When  *  is given, the entry will be demoted to the bottom of the data stack. Examples with different identifier forms data> demote 2\ndata> demote mydata\ndata> demote 853a9f6a-cd5e-4447-a0a4-b4b2793e0a48 Examples with different demotion degrees data> demote mydata\ndata> demote mydata 3\ndata> demote mydata * ?stack load Load a data collection onto the top of the stack The data collection should be given by a path to either: A Data TOML file A folder containing a 'Data.toml' file The path can be optionally preceeded by an position to insert the loaded collection into the stack at. The default behaviour is to put the new collection at the top of the stack. Examples data> load path/to/mydata.toml\ndata> load 2 somefolder/ ?stack remove Remove an entry from the stack An entry can be identified using any of the following: The current position in the stack The name of the data collection The UUID of the data collection Examples data> remove 2\ndata> remove mydata\ndata> remove 853a9f6a-cd5e-4447-a0a4-b4b2793e0a48 ?store   Manipulate the data store\n\n  Subcommand  Action                                         \n  ───────────────────────────────────────────────────────────\n  config      Manage configuration                           \n  expunge     Remove a data collection from the store        \n  fetch       Fetch data storage sources                     \n  gc          Garbage Collect                                \n  stats       Show statistics about the data store           \n  help        Display help text for commands and transformers\n ?store config Manage configuration ?store expunge Remove a data collection from the store Usage data> expunge [collection name or UUID] ?store gc Garbage Collect Scan the inventory and perform a garbage collection sweep. Optionally provide the  -d / --dryrun  flag to prevent file deletion. ?store stats Show statistics about the data store ?help Display help information on the available Data REPL commands\n\nFor convenience, help information can also be accessed via '?', e.g. '?help'.\n\nHelp for data transformers can also be accessed by asking for the help of the\ntransformer name prefixed by ':' (i.e. ':transformer'), and a list of documented\ntransformers can be pulled up with just ':'.\n\nUsage\n=====\n\ndata> help\ndata> help CMD\ndata> help PARENT CMD\ndata> PARENT help CMD\ndata> help :\ndata> help :TRANSFORMER\n"},{"id":199,"pagetitle":"Chain","title":"Chain","ref":"/DataToolkitDocs/common/stable/saveload/chain/#Chain","content":" Chain Chain multiple transformers together Input/Output The  chain  loader can accept any form of input, and produce any form of output. In passes the initial input through a  chain  of other loaders, via the  loader  property. A list of loader driver names can be given to chain together those loaders with no properties. To provide properties, use a TOML array of tables and specify the full ( driver = \"name\", ... ) form. Writing is not currently supported. Usage examples [[iris.loader]]\ndriver = \"chain\"\nloaders = [\"gzip\", \"csv\"] [[iris.loader]]\ndriver = \"chain\"\nloaders = [\"gzip\", { driver = \"tar\", file = \"iris.csv\" }, \"csv\"] [[chained.loader]]\ndriver = \"chain\"\n\n    [[chained.loader.loaders]]\n    driver = \"gzip\"\n\n    [[chained.loader.loaders]]\n    driver = \"csv\"\n\n    [[chained.loader.loaders]]\n    driver = \"julia\"\n    input = \"DataFrame\"\n    path = \"scripts/custom_postprocessing.jl\"\n    type = \"DataFrame\""},{"id":202,"pagetitle":"Compression","title":"Compression","ref":"/DataToolkitDocs/common/stable/saveload/compression/#Compression","content":" Compression Load and write a variety of compressed formats Description This is a collection of drivers which enable transparent compression and decompression of data, specifically the following eponymous drivers: gzip zlib deflate bzip2 xz zstd Input/output It is assumed that during reading decompression is the desired operation, compression desired when writing. In both cases,  IO  \\to  IO  is the recommended pair of input/output formats, but  IO  or  String  to  Vector{UInt8}  or  String  are also supported. Required packages CodecZlib , for the following drivers: gzip zlib deflate CodecBzip2  for the  bzip2  driver CodecXz  for the  xz  driver CodecZstd  for the  zstd  driver Usage examples [[iris-raw.loader]]\ndriver = \"gzip\""},{"id":205,"pagetitle":"CSV","title":"CSV","ref":"/DataToolkitDocs/common/stable/saveload/csv/#CSV","content":" CSV Parse and serialize CSV data While this is the  csv  driver, any format that  CSV.jl  can work with is supported (as this is merely a thin layer around  CSV.jl ) Input/output The  csv  driver expects data to be provided via  IO . By default this driver announces support for parsing to three data types: DataFrame Matrix CSV.File Other  Tables  compatible types are of course supported, and can be used directly (i.e. without having to use the  CSV.File  result) by specifying the type with the  type  transformer keyword. When writing, any type compatible with  CSV.write  can be used directly, to any storage backend supporting  IO . Required packages CSV Parameters args : keyword arguments to be provided to  CSV.File , see https://csv.juliadata.org/stable/reading.html#CSV.File. As a quick-reference, some arguments of particular interest are: header : Either, the row number to parse for column names the list of column names delim : the column delimiter types : a single type, or vector of types to be used for the columns Usage examples [[iris.loader]]\ndriver = \"csv\"\n\n    [iris.loader.args]\n    key = value..."},{"id":208,"pagetitle":"Delim","title":"Delim","ref":"/DataToolkitDocs/common/stable/saveload/delim/#Delim","content":" Delim Parse and serialize delimited data Input/output The  delim  driver expects data to be provided via  IO . It presents the parsed information as a  Matrix , and can write  Matrix  and  Vector  types to an  IO -supporting storage backend. Required packages DelimitedFiles  (the stdlib) Parameters dtype : The element type of the matrix delim : The character used to separate entries eol : The character separating each line of input header : Whether the first row of data should be read as a header skipstart : The number of initial lines of input to ignore skipblanks : Whether to ignore blank lines quotes : Whether to allow quoted strings to contain column and line delimiters Usage examples [[iris.loader]]\ndriver = \"delim\""},{"id":211,"pagetitle":"IO to File","title":"IO to File","ref":"/DataToolkitDocs/common/stable/saveload/iotofile/#IO-to-File","content":" IO to File nothing"},{"id":214,"pagetitle":"JLD2","title":"JLD2","ref":"/DataToolkitDocs/common/stable/saveload/jld2/#JLD2","content":" JLD2"},{"id":215,"pagetitle":"JLD2","title":"Description","ref":"/DataToolkitDocs/common/stable/saveload/jld2/#Description","content":" Description The  jld2  driver enables the  parsing  and  serialisation  of JSON data."},{"id":216,"pagetitle":"JLD2","title":"Input/output","ref":"/DataToolkitDocs/common/stable/saveload/jld2/#Input/output","content":" Input/output The  jld2  driver expects data to be provided via  IO . It will parse to a number of types depending on the input: JSON3.Object JSON3.Array String Number Boolean Nothing If you do not wish to impose any expectations on the parsed type, you can ask for the data of type  Any . When writing, any type compatible with  JSON3.write  can be used directly, with any storage backend supporting  IO ."},{"id":217,"pagetitle":"JLD2","title":"Required packages","ref":"/DataToolkitDocs/common/stable/saveload/jld2/#Required-packages","content":" Required packages JSON3"},{"id":218,"pagetitle":"JLD2","title":"Parameters","ref":"/DataToolkitDocs/common/stable/saveload/jld2/#Parameters","content":" Parameters pretty :: Whether to use  JSON3.pretty  when writing"},{"id":219,"pagetitle":"JLD2","title":"Usage examples","ref":"/DataToolkitDocs/common/stable/saveload/jld2/#Usage-examples","content":" Usage examples [[sample.loader]]\ndriver = \"json\""},{"id":222,"pagetitle":"JPEG","title":"JPEG","ref":"/DataToolkitDocs/common/stable/saveload/jpeg/#JPEG","content":" JPEG Encode and decode JPEG images Input/output The  jpeg  driver expects data to be provided via  IO . It will parse to a  Matrix{<:Colorant} . Required packages JpegTurbo Parameters Reader transpose : Whether to permute the image's width and height dimension scale_ratio : scale the image this ratio in  M/8  with  M ∈ 1:16  (values will be mapped to the closest value) grayscale : Whether to process the image in grayscale Writer transpose : Whether to permute the image's width and height dimension quality : The IJG-scale JPEG quality value, between 0 and 100. Usage examples [[someimage.loader]]\ndriver = \"jpeg\""},{"id":225,"pagetitle":"JSON","title":"JSON","ref":"/DataToolkitDocs/common/stable/saveload/json/#JSON","content":" JSON Parse and serialize JSON data Input/output The  json  driver expects data to be provided via  IO . It will parse to a number of types depending on the input: JSON3.Object JSON3.Array String Number Boolean Nothing If you do not wish to impose any expectations on the parsed type, you can ask for the data of type  Any . When writing, any type compatible with  JSON3.write  can be used directly, with any storage backend supporting  IO . Required packages JSON3 Parameters pretty : Whether to use  JSON3.pretty  when writing Usage examples [[sample.loader]]\ndriver = \"json\""},{"id":228,"pagetitle":"Julia","title":"Julia","ref":"/DataToolkitDocs/common/stable/saveload/julia/#Julia","content":" Julia Load and write data via custom Julia scripts The  julia  driver enables the  parsing  and  serialisation  of arbitrary data to arbitrary information formats and vice versa via custom Julia functions run within the scope of the parent module. Input/output The  julia  driver either accepts /no/ direct input, or accepts input from storage backends of the type specified by the  input  keyword. Thus, the provided functions must take one of the following forms: function (input; kwargs...)\n    # Direct input form.\nend function (kwargs...)\n    # No direct input form.\nend In both cases, additional information can be provided via the  arguments  keyword, which supplies additional keyword arguments to the Julia function invoked. It is worth remembering the special treatment of DataSet strings which are dynamically resolved (see the examples). Writer functions take two arguments, the destination (a handle to the storage backend, usually  IO ) and the information to be serialised. function (destination, info)\n    # Write `info` to `destination`, and return\n    # not-nothing if the operation succeeds.\nend Parameters input : (loading only) The data type required for direct input. path : A local file path, relative to  pathroot  if provided or the directory of the data TOML file. pathroot : The root path to expand  path  against, relative to the directory of the data TOML file. function : The function as a string, inline in the data TOML file. arguments : Arguments to be provided to the called function. Usage examples [[addone.loader]]\ndriver = \"julia\"\ninput = \"Number\"\nfunction = \"n -> n+1\" [[combined.loader]]\ndriver = \"julia\"\npath = \"scripts/mergedata.jl\"\n\n[combined.loader.arguments]\nfoo = \"📇DATASET<<foo::DataFrame>>\"\nbar = \"📇DATASET<<bar::DataFrame>>\"\nbaz = \"📇DATASET<<baz::DataFrame>>\" [[repeated.loader]]\ndriver = \"julia\"\ninput = \"Integer\"\nfunction = \"(n::Integer; data::DataFrame) -> repeat(data, n)\"\narguments = { data = \"📇DATASET<<iris::DataFrame>>\" }"},{"id":231,"pagetitle":"Netpbm","title":"Netpbm","ref":"/DataToolkitDocs/common/stable/saveload/netpbm/#Netpbm","content":" Netpbm Encode and decode NetPBM files Input/output The  netpbm  driver expects data to be provided via  IO . It will parse to a  Matrix{<:Colorant} . Required packages Netpbm Usage examples [[someimage.loader]]\ndriver = \"netpbm\""},{"id":234,"pagetitle":"Passthrough","title":"Passthrough","ref":"/DataToolkitDocs/common/stable/saveload/passthrough/#Passthrough","content":" Passthrough Simply passes on data to/from the storage backend Input/output Identical to that of the storage. Usage examples [[magicvalue.loader]]\ndriver = \"passthrough\""},{"id":237,"pagetitle":"PNG","title":"PNG","ref":"/DataToolkitDocs/common/stable/saveload/png/#PNG","content":" PNG Encode and decode PNG images Input/output The  png  driver expects data to be provided via  IO . It will parse to a  Matrix{<:Colorant} . Required packages PNGFile Parameters Reader gamma : The gamma correction coefficient. expand_paletted : See the PNGFile docs. Writer gamma : The gamma correction coefficient. compression_level : 0-9 compression_strategy : Either the number or string of: 0/\"default\", 1/\"filtered\", 2/\"huffman\", 3/\"rle\" (default), or 4/\"fixed\". filters : Either the number or string of: 0/\"none\", 1/\"sub\", 3/\"average\", 4/\"paeth\" (default) Usage examples [[someimage.loader]]\ndriver = \"png\""},{"id":240,"pagetitle":"QOI","title":"QOI","ref":"/DataToolkitDocs/common/stable/saveload/qoi/#QOI","content":" QOI Encode and decode QOI (Quite Ok Image) files Input/output The  qoi  driver expects data to be provided via  IO . It will parse to a  Matrix{<:Colorant} . Required packages QOI Usage examples [[someimage.loader]]\ndriver = \"qoi\""},{"id":243,"pagetitle":"SQLite","title":"SQLite","ref":"/DataToolkitDocs/common/stable/saveload/sqlite/#SQLite","content":" SQLite Load and write data from/to an SQLite database file Input/output The  sqlite  driver expects to be provided a path to an SQLite database file. By default this driver announces support for parsing to three data types: SQLite.DB DataFrame Any Any valid constructor that can be applied to the results of  DBInterface.execute  will work. Required packages SQLite Parameters Loader and Writer table : The table to act on,  data  by default. Loader only columns : columns to select,  *  by default. query : an SQLite query to run. When provided this overrides the  table  and  columns  parameters. Writer only ifnotexists : see the documentation for  SQLite.load! . analyze : see the documentation for  SQLite.load! . Usage examples [[iris.loader]]\ndriver = \"sqlite\"\ncolumns = [\"sepal_length\", \"species\"]"},{"id":246,"pagetitle":"Tar","title":"Tar","ref":"/DataToolkitDocs/common/stable/saveload/tar/#Tar","content":" Tar Load the contents of a Tarball. Input/output The  zip  driver expects data to be provided via  IO  or a  FilePath . It can load the contents to the following formats: Dict{FilePath, IO} Dict{FilePath, Vector{UInt8}} Dict{FilePath, String} Dict{String, IO} Dict{String, Vector{UInt8}} Dict{String, String} IO  (single file) Vector{UInt8}  (single file) String  (single file) Required packages Tar  (the stdlib) Parameters file : the file in the zip whose contents should be extracted, when producing  IO . Usage examples [[dictionary.loader]]\ndriver = \"tar\"\nfile = \"dictionary.txt\""},{"id":249,"pagetitle":"TIFF","title":"TIFF","ref":"/DataToolkitDocs/common/stable/saveload/tiff/#TIFF","content":" TIFF Encode and decode Tiff files Input/output The  tiff  driver expects data to be provided via  IO . It will parse to a  TiffImages.AbstractTIFF . Required packages TiffImages Usage examples [[someimage.loader]]\ndriver = \"tiff\""},{"id":252,"pagetitle":"XLSX","title":"XLSX","ref":"/DataToolkitDocs/common/stable/saveload/xlsx/#XLSX","content":" XLSX Load Microsoft Excel XML Spreadsheet (XLSX) files Input/output The  xlsx  driver expects data to be provided via a  FilePath , and will provide information as a  Matrix . Required packages XLSX Parameters sheet : the sheet to act on range : the sheet range that should be loaded Usage example [[pleaseno]]\ndriver = \"xlsx\"\nsheet = \"better_formats\"\nrange = \"A1:A999\""},{"id":255,"pagetitle":"Zip","title":"Zip","ref":"/DataToolkitDocs/common/stable/saveload/zip/#Zip","content":" Zip Load the contents of zipped data Input/output The  zip  driver expects data to be provided via  IO  or a  FilePath . It can load the contents to the following formats: Dict{FilePath, IO} , Dict{String, IO} , IO , and an unzipped  FilePath . Required packages ZipFile Parameters file : the file in the zip whose contents should be extracted, when producing  IO . extract : the path that the zip should be extracted to, when producing an unzipped  FilePath . recursive : when extracting to a  FilePath , whether nested zips should be unzipped too. Usage examples [[dictionary.loader]]\ndriver = \"zip\"\nfile = \"dictionary.txt\""},{"id":258,"pagetitle":"Filesystem","title":"Filesystem","ref":"/DataToolkitDocs/common/stable/storage/filesystem/#Filesystem","content":" Filesystem Read and write access to local files Parameters path : The path to the file in question, relative to the  Data.toml  if applicable, otherwise relative to the current working directory. Usage examples [[iris.loader]]\ndriver = \"filesystem\"\npath = \"iris.csv\" [[iris.loader]]\ndriver = \"filesystem\"\npath = \"~/data/iris.csv\""},{"id":261,"pagetitle":"Git","title":"Git","ref":"/DataToolkitDocs/common/stable/storage/git/#Git","content":" Git Access a tarball of a git repository Parameters remote : A remote repository path, e.g.  git://...  or  git@... . revision : A \"tree-ish\" specification of a particular revision to use,  HEAD  by default. See https://git-scm.com/docs/gitrevisions# specifying revisions. path  (optional): A subdirectory of the repository to archive, instead of the entire repository. clone : Whether to use a clone instead of  git archive --remote .  false  by default. Tree-ish forms The particular \"tree-ish\" forms accepted can depend on the remote, but these are the forms that seem to work. Form Examples <describeOutput> v1.7.4.2-679-g3bee7fb <refname> master ,  heads/master ,  refs/heads/master <rev> HEAD ,  v1.5.1 When applied to a local/cloned repository, more forms are possible. Form Examples <sha1> dae86e1950b1277e545cee180551750029cfe735 <refname>@{<date>} master@{yesterday} ,  HEAD@{5 minutes ago} <refname>@{<n>} master@{1} @{<n>} @{1} @{-<n>} @{-1} <refname>@{upstream} master@{upstream} ,  @{u} <rev>~<n> master~3 <rev>^{<type>} v0.99.8^{commit} <rev>^{} v0.99.8^{} <rev>^{/<text>} HEAD^{/fix nasty bug} :/<text> :/fix nasty bug Usage examples [[myrepo.storage]]\ndriver = \"git\"\nremote = \"git@forge.example:user/project.git\"\nrevision = \"v1.4\"\npath = \"subdir/thing\" [[myrepo.storage]]\ndriver = \"git\"\nremote = \"https://forge.example:user/project.git\"\nrevision = \"2b8a2a4390\"\nclone = true"},{"id":264,"pagetitle":"Null","title":"Null","ref":"/DataToolkitDocs/common/stable/storage/null/#Null","content":" Null A special driver sometimes needed for the  julia  loader This driver always produces  Some(nothing) ."},{"id":267,"pagetitle":"Passthrough","title":"Passthrough","ref":"/DataToolkitDocs/common/stable/storage/passthrough/#Passthrough","content":" Passthrough Use a data set as a storage source The  passthrough  storage driver enables dataset redirection by offering the loaded result of another data set as a  read-only  storage transformer. Write capability may be added in future. Parameters source : The identifier of the source dataset to be loaded. Usage examples [[iris2.storage]]\ndriver = \"passthrough\"\nsource = \"iris1\""},{"id":270,"pagetitle":"Raw","title":"Raw","ref":"/DataToolkitDocs/common/stable/storage/raw/#Raw","content":" Raw Access (read/write) values encoded in the data TOML file. The  passthrough  loader is often useful when using this storage driver. Parameters value : The value in question Usage examples [[lifemeaning.storage]]\ndriver = \"raw\"\nvalue = 42 [[parameters.storage]]\ndriver = \"raw\"\nvalue = { a = 3, b = \"*\", c = false }"},{"id":273,"pagetitle":"Web","title":"Web","ref":"/DataToolkitDocs/common/stable/storage/web/#Web","content":" Web Fetch data from the internet This pairs well with the  store  plugin. Required packages Downloads  (part of Julia's stdlib) Parameters url  :: Path to the online data. headers  :: HTTP headers that should be set. timeout  :: Maximum number of seconds to try to download for before abandoning. Usage examples Downloading the data on-demand each time it is accessed. [[iris.storage]]\ndriver = \"web\"\nurl = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\""}]